<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 The long-term distribution of a Markov chain | SCMA469 Actuarial Statistics</title>
  <meta name="description" content="Chapter 11 The long-term distribution of a Markov chain | SCMA469 Actuarial Statistics" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 The long-term distribution of a Markov chain | SCMA469 Actuarial Statistics" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 The long-term distribution of a Markov chain | SCMA469 Actuarial Statistics" />
  
  
  

<meta name="author" content="Pairote Satiracoo" />


<meta name="date" content="2021-08-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="absorption-probabilities-and-expected-time-to-absorption.html"/>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SCMA469</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Down the rabbit-hole</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction to Stochastic Processes</a></li>
<li class="chapter" data-level="3" data-path="examples-of-real-world-processes.html"><a href="examples-of-real-world-processes.html"><i class="fa fa-check"></i><b>3</b> Examples of real world processes</a></li>
<li class="chapter" data-level="4" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html"><i class="fa fa-check"></i><b>4</b> Review of probability theory</a>
<ul>
<li class="chapter" data-level="4.1" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#random-variables"><i class="fa fa-check"></i><b>4.1</b> Random variables</a></li>
<li class="chapter" data-level="4.2" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#probability-distribution"><i class="fa fa-check"></i><b>4.2</b> Probability distribution</a></li>
<li class="chapter" data-level="" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#examples-of-discrete-and-continuous-random-variables"><i class="fa fa-check"></i>Examples of discrete and continuous random variables</a></li>
<li class="chapter" data-level="4.3" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#conditional-probability"><i class="fa fa-check"></i><b>4.3</b> Conditional probability</a></li>
<li class="chapter" data-level="" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#law-of-total-probability"><i class="fa fa-check"></i>Law of total probability</a></li>
<li class="chapter" data-level="4.4" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#conditional-distribution-and-conditional-expectation"><i class="fa fa-check"></i><b>4.4</b> Conditional distribution and conditional expectation</a></li>
<li class="chapter" data-level="4.5" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#central-limit-theorem"><i class="fa fa-check"></i><b>4.5</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#central-limit-theorem-1"><i class="fa fa-check"></i>Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="stochastic-processes.html"><a href="stochastic-processes.html"><i class="fa fa-check"></i><b>5</b> Stochastic processes</a>
<ul>
<li class="chapter" data-level="5.1" data-path="stochastic-processes.html"><a href="stochastic-processes.html#classification-of-stochastic-processes"><i class="fa fa-check"></i><b>5.1</b> Classification of stochastic processes</a></li>
<li class="chapter" data-level="5.2" data-path="stochastic-processes.html"><a href="stochastic-processes.html#random-walk-an-introductory-example"><i class="fa fa-check"></i><b>5.2</b> Random walk: an introductory example</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html"><i class="fa fa-check"></i><b>6</b> Discrete-time Markov chains</a>
<ul>
<li class="chapter" data-level="6.1" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#one-step-transition-probabilities"><i class="fa fa-check"></i><b>6.1</b> One-step transition probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="the-chapman-kolmogorov-equations.html"><a href="the-chapman-kolmogorov-equations.html"><i class="fa fa-check"></i><b>7</b> The Chapman-Kolmogorov equations</a>
<ul>
<li class="chapter" data-level="7.1" data-path="the-chapman-kolmogorov-equations.html"><a href="the-chapman-kolmogorov-equations.html#the-chapman-kolmogorov-equations-and-n-step-transition-probabilities"><i class="fa fa-check"></i><b>7.1</b> The Chapman-Kolmogorov equations and <span class="math inline">\(n\)</span>-step transition probabilities</a></li>
<li class="chapter" data-level="7.2" data-path="the-chapman-kolmogorov-equations.html"><a href="the-chapman-kolmogorov-equations.html#distribution-of-x_n"><i class="fa fa-check"></i><b>7.2</b> Distribution of <span class="math inline">\(X_n\)</span></a></li>
<li class="chapter" data-level="7.3" data-path="the-chapman-kolmogorov-equations.html"><a href="the-chapman-kolmogorov-equations.html#joint-distribution"><i class="fa fa-check"></i><b>7.3</b> Joint Distribution</a></li>
<li class="chapter" data-level="7.4" data-path="the-chapman-kolmogorov-equations.html"><a href="the-chapman-kolmogorov-equations.html#random-walk-with-absorbing-and-reflecting-barriers"><i class="fa fa-check"></i><b>7.4</b> Random walk with absorbing and reflecting barrier(s)</a></li>
<li class="chapter" data-level="7.5" data-path="the-chapman-kolmogorov-equations.html"><a href="the-chapman-kolmogorov-equations.html#an-example-of-nonhomogeneous-markov-chain"><i class="fa fa-check"></i><b>7.5</b> An example of nonhomogeneous Markov chain</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="simulation.html"><a href="simulation.html"><i class="fa fa-check"></i><b>8</b> Simulation</a>
<ul>
<li class="chapter" data-level="8.1" data-path="simulation.html"><a href="simulation.html#monte-carlo-methods"><i class="fa fa-check"></i><b>8.1</b> Monte Carlo Methods</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="classification-of-states.html"><a href="classification-of-states.html"><i class="fa fa-check"></i><b>9</b> Classification of states</a></li>
<li class="chapter" data-level="10" data-path="absorption-probabilities-and-expected-time-to-absorption.html"><a href="absorption-probabilities-and-expected-time-to-absorption.html"><i class="fa fa-check"></i><b>10</b> Absorption probabilities and expected time to absorption</a>
<ul>
<li class="chapter" data-level="10.1" data-path="absorption-probabilities-and-expected-time-to-absorption.html"><a href="absorption-probabilities-and-expected-time-to-absorption.html#first-step-analysis"><i class="fa fa-check"></i><b>10.1</b> First step analysis</a></li>
<li class="chapter" data-level="10.2" data-path="absorption-probabilities-and-expected-time-to-absorption.html"><a href="absorption-probabilities-and-expected-time-to-absorption.html#the-expected-time-to-absorption"><i class="fa fa-check"></i><b>10.2</b> The expected time to absorption</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="the-long-term-distribution-of-a-markov-chain.html"><a href="the-long-term-distribution-of-a-markov-chain.html"><i class="fa fa-check"></i><b>11</b> The long-term distribution of a Markov chain</a>
<ul>
<li class="chapter" data-level="11.1" data-path="the-long-term-distribution-of-a-markov-chain.html"><a href="the-long-term-distribution-of-a-markov-chain.html#stationary-and-limiting-distributions-for-a-single-closed-class"><i class="fa fa-check"></i><b>11.1</b> Stationary and limiting distributions for a single closed class</a>
<ul>
<li class="chapter" data-level="" data-path="the-long-term-distribution-of-a-markov-chain.html"><a href="the-long-term-distribution-of-a-markov-chain.html#stationary-distributions"><i class="fa fa-check"></i>Stationary distributions</a></li>
<li class="chapter" data-level="" data-path="the-long-term-distribution-of-a-markov-chain.html"><a href="the-long-term-distribution-of-a-markov-chain.html#proportion-of-time-in-each-state"><i class="fa fa-check"></i>Proportion of Time in Each State</a></li>
<li class="chapter" data-level="" data-path="the-long-term-distribution-of-a-markov-chain.html"><a href="the-long-term-distribution-of-a-markov-chain.html#the-method-of-finding-the-stationary-distribution"><i class="fa fa-check"></i>The method of finding the stationary distribution</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="the-long-term-distribution-of-a-markov-chain.html"><a href="the-long-term-distribution-of-a-markov-chain.html#sufficient-conditions-for-the-long-run-behaviour-of-a-markov-chain"><i class="fa fa-check"></i><b>11.2</b> Sufficient conditions for the long-run behaviour of a Markov chain</a></li>
<li class="chapter" data-level="" data-path="the-long-term-distribution-of-a-markov-chain.html"><a href="the-long-term-distribution-of-a-markov-chain.html#limiting-distributions"><i class="fa fa-check"></i>Limiting distributions</a></li>
<li class="chapter" data-level="" data-path="the-long-term-distribution-of-a-markov-chain.html"><a href="the-long-term-distribution-of-a-markov-chain.html#main-result"><i class="fa fa-check"></i>Main result</a>
<ul>
<li class="chapter" data-level="" data-path="the-long-term-distribution-of-a-markov-chain.html"><a href="the-long-term-distribution-of-a-markov-chain.html#applications-of-markov-chains-to-ncd-systems"><i class="fa fa-check"></i>Applications of Markov chains to NCD systems</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">SCMA469 Actuarial Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-long-term-distribution-of-a-markov-chain" class="section level1" number="11">
<h1><span class="header-section-number">Chapter 11</span> The long-term distribution of a Markov chain</h1>
<p>In this section, we present another important property concerning
limiting behaviour of <span class="math inline">\(P^n\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> and hence the
long-term distribution of a Markov chain satisfying some certain
conditions. In particular, some Markov chains will converge to an
equilibrium (limiting) distribution, which is independent of its initial
state.</p>
<p>We also assume that the Markov chain with <strong>a single closed class</strong> <span class="math inline">\(S\)</span>.</p>
<div id="stationary-and-limiting-distributions-for-a-single-closed-class" class="section level2" number="11.1">
<h2><span class="header-section-number">11.1</span> Stationary and limiting distributions for a single closed class</h2>
<div id="stationary-distributions" class="section level3 unnumbered">
<h3>Stationary distributions</h3>
<p>Throughout this section, we consider a Markov chain whose transition
probability matrix is <span class="math inline">\(P\)</span> and state space <span class="math inline">\(S\)</span> is a single close class.
Then <span class="math inline">\(S\)</span> is necessarily closed and hence irreducible.</p>
<p>A probability distribution <span class="math inline">\(\boldsymbol{\pi} = (\pi)_{i \in S}\)</span> on <span class="math inline">\(S\)</span>
is <strong>stationary</strong> if the following conditions hold (here
<span class="math inline">\(\boldsymbol{\pi}\)</span> is a row vector):</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\pi_j = \sum_{i \in S} \pi_i p_{ij}\)</span> or equivalently
<span class="math inline">\(\boldsymbol{\pi} P =\boldsymbol{\pi}\)</span>,</p></li>
<li><p><span class="math inline">\(\pi_j \ge 0\)</span>,</p></li>
<li><p><span class="math inline">\(\sum_{j \in S} \pi_j = 1\)</span>.</p></li>
</ol>
<!-- -->
<ol style="list-style-type: decimal">
<li><p>For any stationary distribution <span class="math inline">\(\boldsymbol{\pi}\)</span>, for all
<span class="math inline">\(n \ge 1\)</span>, <span class="math display">\[\boldsymbol{\pi}  P^n  =\boldsymbol{\pi}.\]</span> Therefore,
if we take <span class="math inline">\(\boldsymbol{\pi}\)</span> as the initial probability
distribution, i.e. <span class="math inline">\(\Pr(X_0 = i) = \pi_i\)</span>, then then the
distribution of <span class="math inline">\(X_n\)</span> is also <span class="math inline">\(\boldsymbol{\pi}\)</span> , i.e.
<span class="math inline">\(\Pr(X_n = i) = \pi_i\)</span></p></li>
<li><p>The probability distribution <span class="math inline">\(\boldsymbol{\pi}\)</span> is said to be an
invariant probability distribution.</p></li>
<li><p>The most important property concerning the stationary
distribution(which will be made formal) is that it gives the
<strong>long-term (limiting) distribution</strong> of a Markov chain. In
addition, <span class="math inline">\(\pi_j\)</span> also gives <strong>the long run mean fraction of time</strong>
that the process <span class="math inline">\(\{X_n\}\)</span> is in state <span class="math inline">\(j\)</span>.</p></li>
</ol>
<div class="example">
<p><span id="exm:exampleStationary" class="example"><strong>Example 11.1  </strong></span><strong>Example 22</strong>. <em>A Markov chain <span class="math inline">\(X_0, X_1, \ldots\)</span> on states <span class="math inline">\(\{1,2\}\)</span>
has the following transition matrix <span class="math display">\[P = \begin{bmatrix}
    1-a &amp; a   \\
    b &amp; 1-b   \\
    %\vdots &amp; \vdots &amp; \vdots  &amp; \vdots \\
    %p_{d1} &amp; p_{d2} &amp; p_{d3} &amp; \dots  &amp; p_{dn}
\end{bmatrix},\]</span> where <span class="math inline">\(0 &lt; a,b &lt; 1.\)</span></em></p>
<ol style="list-style-type: decimal">
<li><p><em>Show that <span class="math display">\[P^n = \frac{1}{a+b} \begin{bmatrix}
    b &amp; a   \\
    b &amp; a   \\
    %\vdots &amp; \vdots &amp; \vdots  &amp; \vdots \\
    %p_{d1} &amp; p_{d2} &amp; p_{d3} &amp; \dots  &amp; p_{dn}
\end{bmatrix} +
\frac{(1-a-b)^n}{a+b} \begin{bmatrix}
    a &amp; -a   \\
    -b &amp; b   \\
    %\vdots &amp; \vdots &amp; \vdots  &amp; \vdots \\
    %p_{d1} &amp; p_{d2} &amp; p_{d3} &amp; \dots  &amp; p_{dn}
\end{bmatrix}.\]</span></em></p></li>
<li><p><em>Show that the stationary probability distribution is
<span class="math display">\[\boldsymbol{\pi} = \left( \frac{b}{a+b},  \frac{a}{a+b} \right)  .\]</span></em></p></li>
<li><p><em>Show that
<span class="math display">\[\lim_{n \rightarrow \infty} p_{ij}^{(n)} = \pi_j &gt; 0 , \text{ for }j \in \{1,2\}.\]</span></em></p></li>
</ol>
</div>
</div>
<div id="proportion-of-time-in-each-state" class="section level3 unnumbered">
<h3>Proportion of Time in Each State</h3>
<p>The limiting distribution provides the long-term behaviour of the Markov
chain, i.e. it is the long-term probability that a Markov chain hits
each state. In this section, it can be shown that it also gives the
long-term proportion of time that the chain visits each state. Let us
consider a Markov chain <span class="math inline">\(X_0, X_1, \ldots\)</span> whose transition probability
matrix is <span class="math inline">\(P\)</span> and its limiting distribution is <span class="math inline">\(\boldsymbol{\pi}\)</span>. Note
that the limiting distribution for the Markov chain satisfies
<span class="math display">\[\lim_{n \rightarrow \infty}  p^{(n)}_{ij} =  \pi_j.\]</span></p>
<p>For each state <span class="math inline">\(j\)</span>, define indicator random variable <span class="math display">\[I_k =
    \begin{cases*}
      1, &amp; if $X_k = j$ \\
      0,  &amp; otherwise,
    \end{cases*}\]</span> for <span class="math inline">\(k = 0,1,\ldots\)</span>. Hence, the number of times that
the Markov chain visits <span class="math inline">\(j\)</span> in the first <span class="math inline">\(n\)</span> steps is given by
<span class="math inline">\(\sum_{k =0}^{n-1} I_k\)</span> and the expected long-term proportion of time
that the chain visits state <span class="math inline">\(j\)</span> given that its initial state is <span class="math inline">\(i\)</span> is
<span class="math display">\[\begin{aligned}
    \lim_{n \rightarrow \infty} \mathrm{E}\left(\frac{1}{n}  \sum_{k =0}^{n-1} I_k \, |\, X_0 = i \right) &amp;= 
    \lim_{n \rightarrow \infty}  \frac{1}{n}  \sum_{k =0}^{n-1} \mathrm{E}( I_k \, |\, X_0 = i ) \\
    &amp;= \lim_{n \rightarrow \infty}  \frac{1}{n}  \sum_{k =0}^{n-1} \Pr(X_k = j | X_0 = i)\\
    &amp;= \lim_{n \rightarrow \infty}  \frac{1}{n}  \sum_{k =0}^{n-1} p^{(k)}_{ij}\\
    &amp;= \lim_{n \rightarrow \infty}  p^{(n)}_{ij} =  \pi_j.\end{aligned}\]</span></p>
<p>Here we use the fact that if the sequence of numbers converges to a
limit, i.e. <span class="math inline">\(x_n \rightarrow x\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>, then the
sequence of partial averages also converges to that limit, i.e.
<span class="math inline">\((x_1 + x_2 + \cdots x_n)/n \rightarrow x\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.
This result is known as Cesaro’s lemma.</p>
<div class="example">
<p><span id="exm:weatherExample2" class="example"><strong>Example 11.2  </strong></span><strong>Example 23</strong>. <em>Recall from
Example <a href="#weather" reference-type="ref" reference="weather">Example 9</a>,
the simple weather pattern can be classified into three types including
rainy (<span class="math inline">\(R\)</span>), cloudy (<span class="math inline">\(C\)</span>) and sunny (<span class="math inline">\(S\)</span>). The weather is observed daily
and can be modelled by the Markov transition matrix
<span class="math display">\[P = \begin{bmatrix}
    0.7 &amp; 0.2 &amp; 0.1    \\
    0.75 &amp; 0.15 &amp; 0.1   \\
   0.2 &amp; 0.4 &amp; 0.4   \\
    %p_{d1} &amp; p_{d2} &amp; p_{d3} &amp; \dots  &amp; p_{dn}
\end{bmatrix}.\]</span> It can be shown that the stationary distribution and
also the limiting distribution of the Markov chain is
<span class="math display">\[\boldsymbol{\pi} = (94/147, 32/147,    1/7)\]</span> which gives the
proportions of visits to rainy, cloudy and sunny states are 94/147,
32/147, 1/7, respectively.</em></p>
</div>
</div>
<div id="the-method-of-finding-the-stationary-distribution" class="section level3 unnumbered">
<h3>The method of finding the stationary distribution</h3>
<p>To find the stationary distribution, we simply solve the linear
equations <span class="math inline">\(\boldsymbol{\pi} P =\boldsymbol{\pi}\)</span> (note that one of the
equations can be discarded), together with the condition
<span class="math inline">\(\sum_{j \in S} \pi_j = 1\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-25" class="example"><strong>Example 11.3  </strong></span><strong>Example 24</strong>. <em>For the NCD process in Example
<a href="#NCD" reference-type="ref" reference="NCD">Example 2</a>, the Markov
chain has the following transition probability matrix
<span class="math display">\[P = \begin{bmatrix}
    1- p &amp; p &amp; 0    \\
    1-p &amp; 0 &amp; p   \\
    0 &amp; 1-p &amp; p    \\
\end{bmatrix}.\]</span> Find the stationary probability distribution of this
chain.</em></p>
</div>
<p>Denote the stationary probability distribution by
<span class="math inline">\(\boldsymbol{\pi} = (\pi_1, \pi_2, \pi_3)\)</span>. From
<span class="math inline">\(\boldsymbol{\pi} P =\boldsymbol{\pi}\)</span> and
<span class="math inline">\(\pi_1 + \pi_2 + \pi_3 = 1\)</span>,
<span class="math display">\[(\pi_1,  \pi_2, \pi_3)  \begin{bmatrix}
    1- p &amp; p &amp; 0    \\
    1-p &amp; 0 &amp; p   \\
    0 &amp; 1-p &amp; p    \\
\end{bmatrix} =  (\pi_1,  \pi_2, \pi_3),\]</span> which is equivalent to
<span class="math display">\[\begin{aligned}
(1- p)\pi_1 + (1- p)\pi_2 &amp;= \pi_1 \\
p\pi_1 + (1- p)\pi_3 &amp;= \pi_2 \\
p\pi_2 + p\pi_3 &amp;= \pi_3 \end{aligned}\]</span></p>
<p>By discarding one of the equations and adding the condition that
<span class="math inline">\(\pi_1 + \pi_2 + \pi_3 = 1\)</span>, one can solve for
<span class="math inline">\(\pi_1, \pi_2, \pi_3\)</span>:
<span class="math display">\[\pi_1 = \frac{(p-1)^2}{p^2 - p + 1},  \quad \pi_2 = \frac{-p^2 + p}{p^2 - p + 1},  \quad \pi_3 = \frac{p^2}{p^2 - p + 1}.\]</span></p>
<ol style="list-style-type: decimal">
<li>In the above two examples, it can be shown that
<span class="math display">\[\lim_{n \rightarrow \infty} p_{ij}^{(n)} = \pi_j &gt; 0 , \text{ for }j \in S,\]</span>
or, in terms of the Markov chain <span class="math inline">\(\{X_n\}\)</span>,
<span class="math display">\[\lim_{n \rightarrow \infty} \Pr(X_n = j | X_0 = i) = \pi_j &gt; 0, \text{ for }j \in S.\]</span>
This means that in the long run (as <span class="math inline">\(n\rightarrow \infty\)</span>), the
probability of finding Markov chain in state <span class="math inline">\(j\)</span> is approximately
<span class="math inline">\(\pi_j\)</span> <strong>no matter in which state the chain began at time 0</strong>. This
property holds for some Markov chains which satisfy <strong>"certain
conditions"</strong>.</li>
</ol>
</div>
</div>
<div id="sufficient-conditions-for-the-long-run-behaviour-of-a-markov-chain" class="section level2" number="11.2">
<h2><span class="header-section-number">11.2</span> Sufficient conditions for the long-run behaviour of a Markov chain</h2>
<p>In what follows, we will establish a set of sufficient conditions for
the long-run behaviour of a Markov chain. Two important results are
stated without proof.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-26" class="theorem"><strong>Theorem 11.1  </strong></span><strong>Theorem 1</strong>. <em>A Markov chain with a finite state space has at least
one stationary probability distribution.</em></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-27" class="theorem"><strong>Theorem 11.2  </strong></span><strong>Theorem 2</strong>. <em>An irreducible Markov chain with a finite state space
has a unique stationary probability distribution.</em></p>
</div>
<div class="example">
<p><span id="exm:rwInfiniteS" class="example"><strong>Example 11.4  </strong></span><strong>Example 25</strong>. <em>The simple random walks <span class="math inline">\(X_n\)</span> on
<span class="math inline">\(S = \{\ldots, -2, -1, 0,1,2, \ldots \}\)</span> is defined as
<span class="math display">\[X_n = X_0 + \xi_1 + \xi_2 + \ldots + \xi_n,\]</span> where the random
variables <span class="math inline">\(\xi_j\)</span> are independent identically distributed with
<span class="math display">\[\Pr(\xi = 1) = p, \quad \Pr(\xi = -1) = 1- p.\]</span> We can check that this
Markov chain is irreducible. However, the state space <span class="math inline">\(S\)</span> is infinite.
It can be checked directly from the equations
<span class="math inline">\(\boldsymbol{\pi} P =\boldsymbol{\pi}\)</span> that there is <strong>no</strong> stationary
distribution (given as an exercise).</em></p>
</div>
<p>First we know that the entries of a stationary distribution sum to one.
Suppose the contrary that there is a stationary distribution
<span class="math inline">\(\boldsymbol{\pi}\)</span> for the simple random walk. Then by spatial
invariance of the simple random walk, <span class="math inline">\(\pi_i\)</span> is constant for all
<span class="math inline">\(i \in S\)</span> and also <span class="math inline">\(\sum_{i\in S}\pi_i = 1\)</span>, which is impossible because
<span class="math inline">\(S\)</span> is infinite. Hence, there is <strong>no</strong> stationary distribution for the
simple random walk.</p>
</div>
<div id="limiting-distributions" class="section level2 unnumbered">
<h2>Limiting distributions</h2>
<p>One of the important properties of stationary distributions is that the
distribution of the Markov chain satisfying certain conditions converges
to the stationary distribution. This result provides the long-term
behaviour of the Markov chain. In order to state the main result of this
section, we need to introduce another concept, namely the period of a
state.</p>
<p>A state <span class="math inline">\(i\)</span> is said to be <strong>periodic</strong> with period <span class="math inline">\(d &gt; 1\)</span> if a return
to <span class="math inline">\(i\)</span> is possible only in a number of steps that is a multiple of <span class="math inline">\(d\)</span>.
Equivalently, the period <span class="math inline">\(d\)</span> is the greatest common divisor of all
integers <span class="math inline">\(n\)</span> for which <span class="math inline">\(p^{(n)}_{ii} &gt; 0\)</span>. If the greatest common
divisor is 1, the state has period 1 and is said to be <strong>aperiodic</strong>.</p>
<p>A Markov chain in which each state has period 1 is called <strong>aperiodic</strong>.
Most Markov chains in applications are aperiodic.</p>
<div class="example">
<p><span id="exm:unlabeled-div-28" class="example"><strong>Example 11.5  </strong></span><strong>Example 26</strong>. <em>Is the NCD system in
Example <a href="#NCD" reference-type="ref" reference="NCD">Example 2</a>
aperiodic?</em></p>
</div>
<p>The entire state space of the NCD system is a single class, and is
necessarily closed. Note also that <span class="math inline">\(p_{00} &gt; 0\)</span> (and also <span class="math inline">\(p_{22} &gt; 0\)</span>),
i.e. the state 0 (and state 2) has an arrow back to itself.
Consequently, it is aperiodic because a return to this state is possible
in any number of steps (or the system can remain in this state in any
length of time).</p>
<p>Similarly, a return to state 1 is possible in <span class="math inline">\(2,3, \ldots\)</span> steps.
Therefore, the NCD system is aperiodic.</p>
<ol style="list-style-type: decimal">
<li><p>Periodicity is a class property, i.e. all states in one class have
the same period (or if <span class="math inline">\(i \leftrightarrow j\)</span>, then <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> have
the same period).</p></li>
<li><p>The Markov chain <span class="math inline">\(\{X_n\}_{n\ge 0}\)</span> is aperiodic if and only if
there exists some <span class="math inline">\(n &gt;0\)</span> such that
<span class="math display">\[p^{(n)}_{ij} &gt; 0 \text{ for all } i,j \in S.\]</span> Such Markov chain
is also called <strong>regular</strong>.</p></li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-29" class="example"><strong>Example 11.6  </strong></span><strong>Example 27</strong>. <em>In the random walk model on a finite state space
<span class="math inline">\(S = \{0,1,\ldots, N\}\)</span> with absorbing boundaries in
Example <a href="#simpleRW" reference-type="ref" reference="simpleRW">Example 12</a>, determine the period of each state.</em></p>
</div>
<p>The transition diagram and the transition matrix of the simple random
walk with absorbing boundaries (states) are given as follows:</p>
<p>The simple random walk with absorbing boundaries has three classes,
<span class="math inline">\(O = \{1, 2, \ldots, N - 1\}\)</span> is non-closed class, <span class="math inline">\(C_1 = {0}\)</span> and
<span class="math inline">\(C_2 = {N}\)</span> are two closed classes. For each state <span class="math inline">\(i\)</span> in <span class="math inline">\(O\)</span>,
<span class="math inline">\(p^{(2n)}_{ii} &gt; 0\)</span> and <span class="math inline">\(p^{(2n+1)}_{ii} = 0\)</span> for <span class="math inline">\(n = 1,2,\ldots\)</span>.
Therefore, the state <span class="math inline">\(i\)</span> in this open communication class has period 2.</p>
<p>On the other hand, <span class="math inline">\(p_{00} = 1\)</span> (and also <span class="math inline">\(p_{NN} = 1\)</span>) and hence, the
states <span class="math inline">\(0\)</span> and <span class="math inline">\(N\)</span> are aperiodic because a return to each of these
states is possible in any number of steps (or the system can remain in
this state in any length of time).</p>
<div class="example">
<p><span id="exm:unlabeled-div-30" class="example"><strong>Example 11.7  </strong></span><strong>Example 28</strong>. <em>In the random walk model on an infinite state space
<span class="math inline">\(S = \{\ldots,-2,-1,0,1,2, \ldots \}\)</span> in
Example <a href="#rwInfiniteS" reference-type="ref" reference="rwInfiniteS">Example 25</a>, determine the period of each state.</em></p>
</div>
<p>The entire state space is a single class. Note also that
<span class="math inline">\(p^{(2n)}_{ii} &gt; 0\)</span> and <span class="math inline">\(p^{(2n+1)}_{ii} = 0\)</span> for <span class="math inline">\(n = 1,2,\ldots\)</span>.
Therefore each state in the random walk on an infinite set <span class="math inline">\(S\)</span> is
periodic with period 2.</p>
<div class="example">
<p><span id="exm:unlabeled-div-31" class="example"><strong>Example 11.8  </strong></span><strong>Example 29</strong>. <em>Suppose the states of the system are <span class="math inline">\(\{1,2,3,4 \}\)</span> and
the transition matrix is <span class="math display">\[P = \begin{bmatrix}
    0 &amp; 1/2 &amp; 0 &amp; 1/2    \\
    1/4 &amp; 0 &amp; 3/4 &amp; 0   \\
    0 &amp; 1/3 &amp; 0 &amp; 2/3    \\
    1/2 &amp; 0 &amp; 1/2 &amp; 0   \\
\end{bmatrix}.\]</span> Determine the period of each state.</em></p>
</div>
<p>The entire state space is a single class. Note also that
<span class="math inline">\(p^{(2n)}_{ii} &gt; 0\)</span> and <span class="math inline">\(p^{(2n+1)}_{ii} = 0\)</span> for <span class="math inline">\(n = 1,2,\ldots\)</span>.
Therefore each state in the random walk on an infinite set <span class="math inline">\(S\)</span> is
periodic with period 2.</p>
<div class="example">
<p><span id="exm:exampleLongTerm" class="example"><strong>Example 11.9  </strong></span><strong>Example 30</strong>. <em>A Markov chain <span class="math inline">\(X_0, X_1, \ldots\)</span> on states <span class="math inline">\(\{1,2\}\)</span>
has the following transition matrix <span class="math display">\[P = \begin{bmatrix}
    0 &amp; 1   \\
    1 &amp; 0   \\
    %\vdots &amp; \vdots &amp; \vdots  &amp; \vdots \\
    %p_{d1} &amp; p_{d2} &amp; p_{d3} &amp; \dots  &amp; p_{dn}
\end{bmatrix},\]</span></em></p>
<ol style="list-style-type: decimal">
<li><p><em><span id="example1" label="example1"><span class="math display">\[example1\]</span></span> Find the stationary
distribution(s) of this Markov chain.</em></p></li>
<li><p><em>Describe the long-term behaviour of the Markov chain. Does the
distribution of the chain tend to the stationary distribution(s)
found in <a href="the-long-term-distribution-of-a-markov-chain.html#example1" reference-type="ref" reference="example1"><span class="math display">\[example1\]</span></a>.</em></p></li>
</ol>
</div>
<ol style="list-style-type: decimal">
<li><p>To find the stationary distribution
<span class="math inline">\(\boldsymbol{\pi} = (\pi_{1}, \pi_{2})\)</span>, we need to solve
<span class="math display">\[\left(\pi_{1},  \pi_{2}\right) \begin{bmatrix}
    0 &amp; 1   \\
    1 &amp; 0   \\
    %\vdots &amp; \vdots &amp; \vdots  &amp; \vdots \\
    %p_{d1} &amp; p_{d2} &amp; p_{d3} &amp; \dots  &amp; p_{dn}
\end{bmatrix} = \left(\pi_{1},  \pi_{2}\right),\]</span> and
<span class="math display">\[\pi_{1} +  \pi_{2}  = 1.\]</span> This gives
<span class="math inline">\(\boldsymbol{\pi} = (1/2, 1/2).\)</span></p></li>
<li><p>There is an equal chance of being in either state. Note that for any
initial probability distribution
<span class="math inline">\(\boldsymbol{\mu} = (\mu_{1},\mu_{2})\)</span> with <span class="math inline">\(\mu_{1}+\mu_{2}=1\)</span>, we
have</p>
<p><span class="math display">\[\boldsymbol{\mu} \cdot P=\boldsymbol{\mu} \cdot P^{3}=\boldsymbol{\mu} \cdot P^{5}=\ldots
=(\mu_{2},\mu_{1})\]</span> and
<span class="math display">\[\boldsymbol{\mu} P^{2}=\boldsymbol{\mu} P^{4}=\boldsymbol{\mu} P^{6}=\ldots = (\mu_{1},\mu_{2}).\]</span></p>
<p>The process does not settle down to an equilibrium position. Note
also that the chain is not aperiodic, i.e. each state is periodic of
period 2. The process does not conform to stationary in the long
run.</p></li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-32" class="example"><strong>Example 11.10  </strong></span><strong>Example 31</strong>. <em>A Markov chain <span class="math inline">\(X_0, X_1, \ldots\)</span> on states <span class="math inline">\(\{1,2\}\)</span>
has the following transition matrix <span class="math display">\[P = \begin{bmatrix}
    1/3 &amp; 2/3   \\
    2/3 &amp; 1/3   \\
    %\vdots &amp; \vdots &amp; \vdots  &amp; \vdots \\
    %p_{d1} &amp; p_{d2} &amp; p_{d3} &amp; \dots  &amp; p_{dn}
\end{bmatrix},\]</span> Answer the same questions as given in the Example
<a href="#exampleLongTerm" reference-type="ref" reference="exampleLongTerm">Example 30</a></em></p>
</div>
<ol style="list-style-type: decimal">
<li><p>The process is finite and irreducible, so a unique stationary
distribution exists. To find the stationary distribution
<span class="math inline">\(\boldsymbol{\pi} = (\pi_{1}, \pi_{2})\)</span>, we need to solve
<span class="math display">\[\left(\pi_{1},  \pi_{2}\right) \begin{bmatrix}
    1/3 &amp; 2/3   \\
    2/3 &amp; 1/3   \\
    %\vdots &amp; \vdots &amp; \vdots  &amp; \vdots \\
    %p_{d1} &amp; p_{d2} &amp; p_{d3} &amp; \dots  &amp; p_{dn}
\end{bmatrix} = \left(\pi_{1},  \pi_{2}\right),\]</span> and
<span class="math display">\[\pi_{1} +  \pi_{2}  = 1.\]</span> Solving the system of linear equations
gives <span class="math inline">\(\boldsymbol{\pi} = (1/2, 1/2).\)</span></p></li>
<li><p>The chain is aperiodic. Therefore, according to the results from
Example <a href="#exampleStationary" reference-type="ref" reference="exampleStationary">Example 22</a>, if follows that
<span class="math display">\[\lim_{n \rightarrow \infty} p_{i1}^{(n)} = 1/2 &gt; 0 , \text{ and }  \lim_{n \rightarrow \infty} p_{i2}^{(n)} = 1/2,\]</span>
which is independent of <span class="math inline">\(i\)</span>. This is contrast to the process given
in Example <a href="#exampleLongTerm" reference-type="ref" reference="exampleLongTerm">Example 30</a>, i.e. the process in this example
reaches the stationary probability distribution in the long run.</p></li>
</ol>
</div>
<div id="main-result" class="section level2 unnumbered">
<h2>Main result</h2>
<p>The main result in this section can be stated as follows:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-33" class="theorem"><strong>Theorem 11.3  </strong></span><strong>Theorem 3</strong>. <em>Let <span class="math inline">\(P\)</span> be the transition probability matrix of a
homogeneous discrete-time Markov chain <span class="math inline">\(\{X_n \}_{n \ge 0}\)</span>. If the
Markov chain is</em></p>
<ul>
<li><p><em>finite,</em></p></li>
<li><p><em>irreducible and</em></p></li>
<li><p><em>aperiodic,</em></p></li>
</ul>
<p><em>then there is the unique probability distribution
<span class="math inline">\(\boldsymbol{\pi} = (\pi_j)_{j\in S}\)</span> such that
<span class="math display">\[\lim_{n \rightarrow \infty} p^{(n)}_{ij} = \pi_j &gt; 0, \text{ for any } j \in S\]</span>
and <span class="math display">\[\sum_{j \in S} \pi_j = 1,\]</span> and this distribution is independent
of the initial state. Such probability distribution <span class="math inline">\(\boldsymbol{\pi}\)</span>
is called the <strong>limiting probability distribution</strong>. In addition, the
limiting distribution <span class="math inline">\(\boldsymbol{\pi} = (\pi_j)_{j\in S}\)</span> is the
stationary probability distribution of the Markov chain, i.e. it also
satisfies <span class="math inline">\(\boldsymbol{\pi} P =\boldsymbol{\pi}\)</span>.</em></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-34" class="example"><strong>Example 11.11  </strong></span><strong>Example 32</strong>. <em>Recall the Markov chain as given in
Example <a href="#exampleStationary" reference-type="ref" reference="exampleStationary">Example 22</a>. The Markov chain is finite, irreducible
and aperiodic. We have also shown that
<span class="math display">\[\boldsymbol{\pi} = \left( \frac{b}{a+b},  \frac{a}{a+b} \right)\]</span> is
the limiting distribution, i.e.
<span class="math display">\[\lim_{n \rightarrow \infty} p_{ij}^{(n)} = \pi_j &gt; 0 , \text{ for }j \in \{1,2\},\]</span>
independent of <span class="math inline">\(i\)</span>. This limiting distribution is also the unique
stationary distribution of the Markov chain, which can be verified by
<span class="math display">\[\left( \frac{b}{a+b},  \frac{a}{a+b} \right) \begin{bmatrix}
    1-a &amp; a   \\
    b &amp; 1-b   \\
\end{bmatrix} =  \left( \frac{b}{a+b},  \frac{a}{a+b} \right),\]</span> where
<span class="math inline">\(0 &lt; a,b &lt; 1.\)</span></em></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-35" class="example"><strong>Example 11.12  </strong></span><strong>Example 33</strong>. <em>The above result can be applied to the NCD system
because it is finite, irreducible and aperiodic. Indeed, there is the
unique limiting probability distribution
<span class="math display">\[\boldsymbol{\pi} = \left(\frac{(p-1)^2}{p^2 - p +1}, \frac{p - p^2}{p^2 - p +1}, \frac{p^2}{p^2 - p +1}\right),\]</span>
which is the stationary distribution of the chain. This gives the
long-term behaviour of the Markov chain, i.e. the probability of finding
the Markov chain in state <span class="math inline">\(j\)</span> is approximately <span class="math inline">\(\pi_j\)</span> independent of
the initial distribution.</em></p>
<p><em>For example, let <span class="math inline">\(p = 0.8\)</span> in the transition probability matrix <span class="math inline">\(P\)</span>.
We compute several powers of <span class="math inline">\(P\)</span> as follows: <span class="math display">\[\begin{aligned}
P &amp;= \begin{bmatrix}
0.2 &amp; 0.8 &amp; 0\\
0.2 &amp; 0 &amp; 0.8\\
0 &amp; 0.2 &amp; 0.8\\
\end{bmatrix}
,  &amp;
P^2 &amp;= \begin{bmatrix}
0.2 &amp; 0.16 &amp; 0.64\\
0.04 &amp; 0.32 &amp; 0.64\\
0.04 &amp; 0.16 &amp; 0.8\\
\end{bmatrix}, \\
P^4 &amp;= \begin{bmatrix}
0.072 &amp; 0.1856 &amp; 0.7424\\
0.0464 &amp; 0.2112 &amp; 0.7424\\
0.0464 &amp; 0.1856 &amp; 0.768\\
\end{bmatrix}
,  &amp;
P^8 &amp;= \begin{bmatrix}
0.0482432 &amp; 0.1903514 &amp; 0.7614054\\
0.04758784 &amp; 0.1910067 &amp; 0.7614054\\
0.04758784 &amp; 0.1903514 &amp; 0.7620608\\
\end{bmatrix} \\
P^{16} &amp;= \begin{bmatrix}
0.04761946 &amp; 0.1904761 &amp; 0.7619044\\
0.04761903 &amp; 0.1904765 &amp; 0.7619044\\
0.04761903 &amp; 0.1904761 &amp; 0.7619049\\
\end{bmatrix}
,  &amp;
P^{32} &amp;= \begin{bmatrix}
0.04761905 &amp; 0.1904762 &amp; 0.7619048\\
0.04761905 &amp; 0.1904762 &amp; 0.7619048\\
0.04761905 &amp; 0.1904762 &amp; 0.7619048\\
\end{bmatrix}.\end{aligned}\]</span></em></p>
</div>
<p>The limiting probability distribution is
<span class="math display">\[\lim_{n\rightarrow \infty} P^n=
 \begin{bmatrix}
0.04761905 &amp; 0.1904762 &amp; 0.7619048\\
0.04761905 &amp; 0.1904762 &amp; 0.7619048\\
0.04761905 &amp; 0.1904762 &amp; 0.7619048\\
\end{bmatrix}.\]</span></p>
<div id="applications-of-markov-chains-to-ncd-systems" class="section level3 unnumbered">
<h3>Applications of Markov chains to NCD systems</h3>
<div class="example">
<p><span id="exm:unlabeled-div-36" class="example"><strong>Example 11.13  </strong></span><strong>Example 34</strong>. <em>A no-claims discount system for motor insurance has
three levels of discount:</em></p>
<table>
<thead>
<tr class="header">
<th align="center"><em>Level</em></th>
<th align="center"><em>1</em></th>
<th align="center"><em>2</em></th>
<th align="center"><em>3</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><em>Discount</em></td>
<td align="center"><em>0%</em></td>
<td align="center"><em>30%</em></td>
<td align="center"><em>50%</em></td>
</tr>
</tbody>
</table>
<p><em>The rules for moving between these levels are given as follows:</em></p>
<ul>
<li><p><em>Following a claim-free year, move to the next higher level, or
remain at level 3.</em></p></li>
<li><p><em>Following a year with one claim, move to the next lower level, or
remain at level 1.</em></p></li>
<li><p><em>Following a year with two or more claims, move to level 1, or
remain at level 1.</em></p></li>
</ul>
<p><em>A portfolio consists of 10000 policyholders, of which</em></p>
<ul>
<li><p><em>5000 policyholders are classified as good drivers. The number of
claims per year in this group is <span class="math inline">\(\text{Poisson}(0.1)\)</span>.</em></p></li>
<li><p><em>5000 policyholders are classified as bad drivers. The number of
claims per year in this group is <span class="math inline">\(\text{Poisson}(0.2)\)</span>.</em></p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p><em>Calculate <span class="math inline">\(\Pr[N = 0]\)</span>, <span class="math inline">\(\Pr[N = 1]\)</span>, and <span class="math inline">\(\Pr[N \ge 2]\)</span> for each
group.</em></p></li>
<li><p><em>Write down the transition probability matrix of this no-claims
discount system for each group.</em></p></li>
<li><p><em>Calculate the expected number of policyholders at each level for
each group once stability has been achieved.</em></p></li>
<li><p><em>Calculate the expected premium income per driver from each group
once stability has been achieved.</em></p></li>
<li><p><em>Calculate the ratio of the expected premium income per driver from
the group of good drivers to that from the group of bad drivers once
stability has been achieved.</em></p></li>
<li><p><em>Comments on the results obtained. Does this NCD system encourage
good driving?</em></p></li>
</ol>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="absorption-probabilities-and-expected-time-to-absorption.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/pairote-sat/SCMA469/edit/master/03-race.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/pairote-sat/SCMA469/blob/master/03-race.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
