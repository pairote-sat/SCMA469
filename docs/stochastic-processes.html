<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Stochastic processes | SCMA469 Actuarial Statistics</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Stochastic processes | SCMA469 Actuarial Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Stochastic processes | SCMA469 Actuarial Statistics" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Pairote Satiracoo" />


<meta name="date" content="2021-08-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="review-of-probability-theory.html"/>
<link rel="next" href="discrete-time-markov-chains.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SCMA469 Actuarial Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction to Stochastic Processes</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#examples-of-real-world-processes"><i class="fa fa-check"></i><b>1.1</b> Examples of real world processes</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html"><i class="fa fa-check"></i><b>2</b> Review of probability theory</a>
<ul>
<li class="chapter" data-level="2.1" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#random-variables"><i class="fa fa-check"></i><b>2.1</b> Random variables</a></li>
<li class="chapter" data-level="2.2" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#probability-distribution"><i class="fa fa-check"></i><b>2.2</b> Probability distribution</a></li>
<li class="chapter" data-level="2.3" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#conditional-probability"><i class="fa fa-check"></i><b>2.3</b> Conditional probability</a></li>
<li class="chapter" data-level="2.4" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#law-of-total-probability"><i class="fa fa-check"></i><b>2.4</b> Law of total probability</a></li>
<li class="chapter" data-level="2.5" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#conditional-distribution-and-conditional-expectation"><i class="fa fa-check"></i><b>2.5</b> Conditional distribution and conditional expectation</a></li>
<li class="chapter" data-level="2.6" data-path="review-of-probability-theory.html"><a href="review-of-probability-theory.html#central-limit-theorem"><i class="fa fa-check"></i><b>2.6</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="stochastic-processes.html"><a href="stochastic-processes.html"><i class="fa fa-check"></i><b>3</b> Stochastic processes</a>
<ul>
<li class="chapter" data-level="3.1" data-path="stochastic-processes.html"><a href="stochastic-processes.html#classification-of-stochastic-processes"><i class="fa fa-check"></i><b>3.1</b> Classification of stochastic processes</a></li>
<li class="chapter" data-level="3.2" data-path="stochastic-processes.html"><a href="stochastic-processes.html#random-walk-an-introductory-example"><i class="fa fa-check"></i><b>3.2</b> Random walk: an introductory example</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html"><i class="fa fa-check"></i><b>4</b> Discrete-time Markov chains</a>
<ul>
<li class="chapter" data-level="4.1" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#one-step-transition-probabilities"><i class="fa fa-check"></i><b>4.1</b> One-step transition probabilities</a></li>
<li class="chapter" data-level="4.2" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#the-chapman-kolmogorov-equations-and-n-step-transition-probabilities"><i class="fa fa-check"></i><b>4.2</b> The Chapman-Kolmogorov equations and <span class="math inline">\(n\)</span>-step transition probabilities</a></li>
<li class="chapter" data-level="4.3" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#distribution-of-x_n"><i class="fa fa-check"></i><b>4.3</b> Distribution of <span class="math inline">\(X_n\)</span></a></li>
<li class="chapter" data-level="4.4" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#joint-distribution"><i class="fa fa-check"></i><b>4.4</b> Joint Distribution</a></li>
<li class="chapter" data-level="4.5" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#random-walk-with-absorbing-and-reflecting-barriers"><i class="fa fa-check"></i><b>4.5</b> Random walk with absorbing and reflecting barrier(s)</a></li>
<li class="chapter" data-level="4.6" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#an-example-of-nonhomogeneous-markov-chain"><i class="fa fa-check"></i><b>4.6</b> An example of nonhomogeneous Markov chain</a></li>
<li class="chapter" data-level="4.7" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#simulation"><i class="fa fa-check"></i><b>4.7</b> Simulation</a></li>
<li class="chapter" data-level="4.8" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#monte-carlo-methods"><i class="fa fa-check"></i><b>4.8</b> Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.9" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#classification-of-states"><i class="fa fa-check"></i><b>4.9</b> Classification of states</a></li>
<li class="chapter" data-level="4.10" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#absorption-probabilities-and-expected-time-to-absorption"><i class="fa fa-check"></i><b>4.10</b> Absorption probabilities and expected time to absorption</a></li>
<li class="chapter" data-level="4.11" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#first-step-analysis"><i class="fa fa-check"></i><b>4.11</b> First step analysis</a></li>
<li class="chapter" data-level="4.12" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#the-expected-time-to-absorption"><i class="fa fa-check"></i><b>4.12</b> The expected time to absorption</a></li>
<li class="chapter" data-level="4.13" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#the-long-term-distribution-of-a-markov-chain"><i class="fa fa-check"></i><b>4.13</b> The long-term distribution of a Markov chain</a></li>
<li class="chapter" data-level="4.14" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#stationary-and-limiting-distributions-for-a-single-closed-class"><i class="fa fa-check"></i><b>4.14</b> Stationary and limiting distributions for a single closed class</a>
<ul>
<li class="chapter" data-level="" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#stationary-distributions"><i class="fa fa-check"></i>Stationary distributions</a></li>
<li class="chapter" data-level="" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#proportion-of-time-in-each-state"><i class="fa fa-check"></i>Proportion of Time in Each State</a></li>
<li class="chapter" data-level="" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#the-method-of-finding-the-stationary-distribution"><i class="fa fa-check"></i>The method of finding the stationary distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.15" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#sufficient-conditions-for-the-long-run-behaviour-of-a-markov-chain"><i class="fa fa-check"></i><b>4.15</b> Sufficient conditions for the long-run behaviour of a Markov chain</a></li>
<li class="chapter" data-level="4.16" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#limiting-distributions"><i class="fa fa-check"></i><b>4.16</b> Limiting distributions</a></li>
<li class="chapter" data-level="4.17" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#main-result"><i class="fa fa-check"></i><b>4.17</b> Main result</a>
<ul>
<li class="chapter" data-level="" data-path="discrete-time-markov-chains.html"><a href="discrete-time-markov-chains.html#applications-of-markov-chains-to-ncd-systems"><i class="fa fa-check"></i>Applications of Markov chains to NCD systems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>5</b> Applications</a>
<ul>
<li class="chapter" data-level="5.1" data-path="applications.html"><a href="applications.html#datacamp-light"><i class="fa fa-check"></i><b>5.1</b> DataCamp Light</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>6</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">SCMA469 Actuarial Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="stochastic-processes" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Stochastic processes</h1>
<p>Evolution of a random process is at least partially random, and each run
the process leads to potentially a different outcome. It is of great
interest to understand or model the behaviour of a random process by
describing how different states, represented by random variables <span class="math inline">\(X\)</span>’s,
evolve in the system over time. Just as probability theory is considered
as the study of mathematical models of random phenomena, the theory of
stochastic processes plays an important role in the study of
time-dependent random phenomena. Stochastic processes can be used to
represent many different random phenomena from different fields such as
science, engineering, finance, and economics.</p>
<p>A <strong>stochastic process</strong> is a collection of random variables
<span class="math inline">\(\{ X_t : t \in T\}\)</span> defined on a common sample space, where</p>
<ul>
<li><p><span class="math inline">\(t\)</span> is a parameter running over some index set <span class="math inline">\(T\)</span>, called the
<strong>time domain</strong>.</p></li>
<li><p>The common sample space of the random variables (the range of
possible values for <span class="math inline">\(X_t\)</span>) denoted by <span class="math inline">\(S\)</span> is called the <strong>state
space</strong> of the process.</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>The set of random variables may be dependent or need not be
identically distributed.</p></li>
<li><p>Techniques used to study stochastic processes depend on whether the
state space or the index set (the time domain) are discrete or
continuous.</p></li>
</ol>
<div id="classification-of-stochastic-processes" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Classification of stochastic processes</h2>
<p>Stochastic processes can be classified on the basis of the nature of
their state space and index set.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Discrete state space with discrete time changes</strong> : No claims
discount (NCD) policy: A car owner purchases a motor insurance
policy for which the premium charged depends on the claim record.
Let <span class="math inline">\(X_t\)</span> denote the discount status of a policyholder with three
levels of discount, i.e. <span class="math inline">\(S = \{0,1,2\}\)</span> corresponding to three
discount levels of 0%, 20% and 40% and the time set is
<span class="math inline">\(T = \{0,1,2,\ldots\}\)</span>. Both time and state space are discrete.</p></li>
<li><p><strong>Discrete state space with continuous time changes</strong> : In a health
insurance system, an insurance company may classify policyholders as
Healthy, Sick or Dead, i.e. <span class="math inline">\(S = \{H, S, D\}\)</span>. The time domain can
be taken as continuous, <span class="math inline">\(T = [0,\infty)\)</span>.</p></li>
<li><p><strong>Continuous state space with continuous time changes</strong> : Let <span class="math inline">\(S_t\)</span>
be the total amount claimed by time <span class="math inline">\(t \in T\)</span> where <span class="math inline">\(T = [0,\infty)\)</span>
and the state space is <span class="math inline">\(\mathbb{R}\)</span>. Both time and state space are
continuous. Some continuous time stochastic process taking value in
a continuous state space will be studied in Risk Analysis and
Credibility course.</p></li>
<li><p><strong>Continuous state space with discrete time changes</strong> : The outcomes
of the above claim process <span class="math inline">\(S_t\)</span> could be recorded continuously,
however, we may choose to model the values only at discrete time,
for e.g. the total claim amounts at the end of each day. This may
due to the limitation of the measurement process (for e.g. expensive
to measure). Hence, the time domain is discrete but the state space
is continuous.</p></li>
</ol>
<p>In case that claim amounts are recorded to the nearest baht or in
satang, i.e. discrete state space, we could also approximate or model
the process by using a discrete state space, rather than continuous.</p>
</div>
<div id="random-walk-an-introductory-example" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Random walk: an introductory example</h2>
<p>One of the simplest examples of a stochastic process is a simple random
walk. Consider a simple model of the price of a stock measured in
baht. For each trading day <span class="math inline">\(n = 0,1,2, \ldots\)</span>, the stock price
increases by 1 baht with probability <span class="math inline">\(p\)</span> or decreases by 1 baht with
probability <span class="math inline">\(q = 1-p\)</span>. Let <span class="math inline">\(X_n\)</span> denote the stock price at day <span class="math inline">\(n\)</span> and
<span class="math inline">\(X_0 = 100\)</span>. This simple model is called a simple random
walk.</p>
<p>In the simple random walk process, time is discrete (as observed at the
end of each day) and the state space is discrete. The stochastic model
has an infinite number of outcomes known as <strong>stochastic realisations or
sample paths</strong>. A <strong>sample path</strong> is then just the sequence of a
particular set of experiments. Graphs of some stochastic realisations of
the simple random walk with <span class="math inline">\(p = 0.5\)</span> and <span class="math inline">\(a = 100\)</span> are shown in Figure
<a href="stochastic-processes.html#fig:SamplePaths" reference-type="ref" reference="fig:SamplePaths">1</a>.</p>
<div class="figure">
<embed src="SamplePaths.pdf" id="fig:SamplePaths" style="width:4in" />
<p class="caption">Some stochastic realisations of the simple random
walk</p>
</div>
<p>We can use R to generate sample paths of this random walk.</p>
<script src=https://cdn.datacamp.com/datacamp-light-latest.min.js></script>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="stochastic-processes.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb4-2"><a href="stochastic-processes.html#cb4-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb4-3"><a href="stochastic-processes.html#cb4-3" aria-hidden="true" tabindex="-1"></a>Z<span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>), <span class="dv">10</span>, <span class="at">replace =</span> T, <span class="at">prob =</span> <span class="fu">c</span>(p, <span class="dv">1</span><span class="sc">-</span> p))</span>
<span id="cb4-4"><a href="stochastic-processes.html#cb4-4" aria-hidden="true" tabindex="-1"></a>Z</span></code></pre></div>
<pre><code>##  [1] -1 -1  1  1 -1  1  1  1  1 -1</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="stochastic-processes.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot(c(100, 100 + cumsum(Z)))</span></span>
<span id="cb6-2"><a href="stochastic-processes.html#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># qqplot(0:10,c(100, 100 + cumsum(Z)))</span></span>
<span id="cb6-3"><a href="stochastic-processes.html#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="stochastic-processes.html#cb6-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb6-5"><a href="stochastic-processes.html#cb6-5" aria-hidden="true" tabindex="-1"></a>y1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">100</span> <span class="sc">+</span> <span class="fu">cumsum</span>(Z))</span>
<span id="cb6-6"><a href="stochastic-processes.html#cb6-6" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> x, <span class="at">y =</span> y1)</span>
<span id="cb6-7"><a href="stochastic-processes.html#cb6-7" aria-hidden="true" tabindex="-1"></a>dat <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y1)) <span class="sc">+</span> </span>
<span id="cb6-8"><a href="stochastic-processes.html#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>A complete description of the simple random walk, observed as a
collection of <span class="math inline">\(n\)</span> random variables at time points
<span class="math inline">\(t_1, t_2, \ldots, t_n\)</span> can be specified by the joint distribution of
<span class="math inline">\(X_{t_1}, X_{t_2}, \ldots, X_{t_n}\)</span>, i.e.
<span class="math display">\[F(x_1, x_2, \ldots, x_n) = \Pr(X_{t_1} \le x_1, X_{t_2} \le x_2, \ldots, X_{t_n} \le x_n).\]</span>
However, the <strong>multidimensional distribution function cannot be easily
written in a closed form</strong> unless the random variables have a
multivariate normal distribution. In practice, it is more convenient to
deal with some stochastic processes via some simple <strong>intermediary
processes</strong> or under some addition assumptions.</p>
<p>In general, a simple random walk <span class="math inline">\(X_n\)</span> is a discrete-time stochastic
process defined by</p>
<ul>
<li><p><span class="math inline">\(X_0 = a\)</span> and</p></li>
<li><p>for <span class="math inline">\(n \ge1\)</span>,
<span class="math display">\[X_n = a + \sum_{i=1}^n  Z_i, \text{ where } Z_i = \begin{cases}
    1, &amp; \text{ with probability } p  \\
    -1, &amp; \text{ with probability } q =  1- p.  
 \end{cases}\]</span></p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>When <span class="math inline">\(p = 1/2\)</span>, the value of the process increases or decreases
randomly by 1 unit with equal probability. In this case, the process
is known as a <strong>symmetric</strong> random walk.</p></li>
<li><p>The (intermediary) process <span class="math inline">\(\{ Z_i : i \in \mathbb{N}\}\)</span> is a
sequence of independent identically distributed (i.i.d.) random
variables. The process <span class="math inline">\(X_t\)</span> themselves are neither independent nor
identically distributed. This process <span class="math inline">\(Z_i\)</span> is also known as <strong>white
noise process</strong>.</p></li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example 3.1  </strong></span><em>Explain why the simple random process <span class="math inline">\(X_n\)</span> is neither
independent nor identically distributed.</em></p>
</div>
<p>Suppose that <span class="math inline">\(X_0 = 100\)</span>. Firstly, we will show that <span class="math inline">\(X_n\)</span> is not
independent. From definition, the process <span class="math inline">\(X_n\)</span> can be written as
<span class="math display">\[X_n = X_{n-1} + Z_n.\]</span> That is, the value of <span class="math inline">\(X_n\)</span> is the previous
value plus a random change of 1 or <span class="math inline">\(-1\)</span>. Therefore, the value of the
process depends on the previous value and they are not independent. For
e.g.,
<span class="math display">\[\Pr(X_2 = 102) &gt; 0 \quad \text{ but } \quad  \Pr(X_2 = 102 | X_1 = 99) = 0.\]</span></p>
<p>The process <span class="math inline">\(X_n\)</span> cannot be identically distributed. For e.g. <span class="math inline">\(X_1\)</span> can
take the values of 99 and 101, while <span class="math inline">\(X_2\)</span> can take three different
values of 98, 100 and 102.</p>
<div class="example">
<p><span id="exm:unlabeled-div-10" class="example"><strong>Example 3.2  </strong></span><em>Let <span class="math display">\[\begin{aligned}
    \mu &amp;= \mathrm{E}[Z_i] \\
    \sigma^2 &amp;= \mathrm{Var}[Z_i] \end{aligned}\]</span> Calculate the
expectation (<span class="math inline">\(\mu\)</span>) and variance (<span class="math inline">\(\sigma^2\)</span>) of the random variable
<span class="math inline">\(Z_i\)</span>.</em></p>
</div>
<p><span class="math display">\[\begin{aligned}
    \mu &amp;= \mathrm{E}[Z_i] = 1\cdot p + (-1) \cdot q = p - q.\\\end{aligned}\]</span>
<span class="math display">\[\begin{aligned}
    \sigma^2 &amp;= \mathrm{Var}[Z_i] \\
            &amp;=\mathrm{E}[Z_i^2] - (\mathrm{E}[Z_i] )^2 \\
            &amp;= 1 - (p-q)^2  = (p+q)^2 - (p-q)^2\\
            &amp;= 4pq. \end{aligned}\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-11" class="example"><strong>Example 3.3  </strong></span><em>Calculate the expectation and variance of the process
<span class="math inline">\(X_n\)</span> at time <span class="math inline">\(n\)</span>.</em></p>
</div>
<p><span class="math display">\[\begin{aligned}
     \mathrm{E}[X_n] &amp;=  \mathrm{E}[a + \sum_{i=1}^n Z_i] = a + n\mu.\end{aligned}\]</span>
<span class="math display">\[\begin{aligned}
    \mathrm{Var}[X_n]&amp;= \mathrm{Var}[a + \sum_{i=1}^n Z_i]  = n \sigma^2.\end{aligned}\]</span>
It should be noted that the variance of <span class="math inline">\(X_n\)</span> increases with time.</p>
<div class="example">
<p><span id="exm:unlabeled-div-12" class="example"><strong>Example 3.4  </strong></span><em>For the random process, calculate
<span class="math display">\[\Pr(X_2 = 98, X_5 = 99 | X_0 = 100).\]</span></em></p>
</div>
<p>The process <span class="math inline">\(X_n\)</span> must decrease on the first two days, which happens
with probability <span class="math inline">\((1-p)^2\)</span>. Independently, it must then increases on
another two days and decrease on one day (not necessarily in that
order), giving three different possibilities. Each of these has
probability <span class="math inline">\(p^2(1-p)\)</span>. So
<span class="math display">\[\Pr(X_2 = 98, X_5 = 99 | X_0 = 100) = (1-p)^2 \cdot 3 p^2(1-p) = 3p^2(1-p)^3.\]</span></p>
<p>In what follows, we will see that exact calculations are possible for
the simple random walk process. Note also that it is sufficient to
understand the behaviour of the random walk when it starts at <span class="math inline">\(X_0 = 0\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-13" class="example"><strong>Example 3.5  </strong></span><em>For the random process with <span class="math inline">\(X_0 = 0\)</span>, <span class="math inline">\(X_{2n}\)</span> is
always even and <span class="math inline">\(X_{2n+1}\)</span> is always odd. Based on the binomial
distribution, show that <span class="math display">\[\begin{aligned}
    \Pr(X_{2n} = 2m) &amp;= {2n \choose n+m} p^{n+m} q^{n-m}, \quad   -n \le m \le n \\
    \Pr(X_{2n+1} = 2m+1) &amp;= {2n + 1 \choose n+m+1} p^{n+m+1} q^{n-m}, \quad   -n-1 \le m \le n.\end{aligned}\]</span></em></p>
</div>
<p>Let <span class="math inline">\(A\)</span> denote the number of <span class="math inline">\(+1\)</span> and <span class="math inline">\(B\)</span> denote the number of <span class="math inline">\(-1\)</span>.
Then <span class="math inline">\(A + B = 2n\)</span> and <span class="math inline">\(X_{2n} = A - B\)</span> (i.e. the position at time <span class="math inline">\(2n\)</span>).
Hence, <span class="math display">\[\begin{aligned}
    \Pr(X_{2n} = 2m) &amp;= \Pr( A - B = 2m) \\
    &amp;= \Pr( A - (2n - A) = 2m)  =   \Pr( 2A - 2n  = 2m) =   \Pr( A   = m + n)\\
    &amp;= {2n \choose n+m} p^{n+m} q^{2n-(n+m)}, \quad   -n \le m \le n \\
    &amp;= {2n \choose n+m} p^{n+m} q^{n-m}, \quad   -n \le m \le n.\end{aligned}\]</span></p>
<!-- # Introduction to Stochastic Processes {#intro} -->
<!-- You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods). -->
<!-- Figures and tables with captions will be placed in `figure` and `table` environments, respectively. -->
<!-- ```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'} -->
<!-- par(mar = c(4, 4, .1, .1)) -->
<!-- plot(pressure, type = 'b', pch = 19) -->
<!-- ``` -->
<!-- Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab). -->
<!-- ```{r nice-tab, tidy=FALSE} -->
<!-- knitr::kable( -->
<!--   head(iris, 20), caption = 'Here is a nice table!', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- ``` -->
<!-- You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015]. -->
<!-- The course will cover the probabilistic framework for stochastic models -->
<!-- of real-world applications with emphasis on actuarial work. We will -->
<!-- illustrate some practical actuarial problems for which we will develop -->
<!-- mathematical models, tools and techniques for analysing and quantifying -->
<!-- the uncertainty of the problems. -->
<!-- Here are some of the examples which will be covered later in the course. -->
<!-- Examples of real world processes -->
<!-- ================================ -->
<!-- ::: {.example} -->
<!-- **Example 1**. *(**No claims discount systems (NCD)**) A well-known -->
<!-- model widely used by auto insurance companies is the **no claims -->
<!-- discount system**, in which an insured receives a discount for a claim -->
<!-- free year, while the insured is penalised by an additional premium when -->
<!-- one or more accidents occur.* -->
<!-- *An example of the NCD system in UK may be structured as follows:* -->
<!--     *Level*     *7*      *6*     *5*     *4*     *3*     *2*     *1* -->
<!--   ----------- -------- ------- ------- ------- ------- ------- ------- -->
<!--    *Premium*   *100%*   *75%*   *65%*   *55%*   *45%*   *40%*   *33%* -->
<!-- *The rules for moving between these levels are as follows:* -->
<!-- -   *For a claim-free year, a policyholder moves down 1 level.* -->
<!-- -   *Levels 4$-$7:* -->
<!--     -   *For every one claim, the policyholder moves up 1 level or -->
<!--         remains at level 7.* -->
<!--     -   *For every two or more claims, move to, or remains at, level 7.* -->
<!-- -   *Levels 2$-$3:* -->
<!--     -   *For every one claim, move up 2 levels.* -->
<!--     -   *For every two claims, move up 4 levels.* -->
<!--     -   *For every three or more claims, move to level 7.* -->
<!-- -   *Level 1:* -->
<!--     -   *For every one claim, move to level 4.* -->
<!--     -   *For every two claims, move to level 6.* -->
<!--     -   *For every three or more claims, move to level 7.* -->
<!-- *The no claims discount system is a form of experience rating consisting -->
<!-- of a finite number of levels (or classes), each with its own premium. -->
<!-- The 7 levels are experience-rated as described above.* -->
<!-- *For the NCD model, questions of interest may include:* -->
<!-- 1.  *For 10,000 policyholders at level 7, estimate the *expected -->
<!--     numbers* at each discount level at a given time, or once stability -->
<!--     has been achieved.* -->
<!-- 2.  *What is the *probability* that a policyholder who is at a specific -->
<!--     discount level (i.e. one of the levels 1-6) has no discount after 2 -->
<!--     years?* -->
<!-- 3.  *What is the *distribution* of being in one of the levels at time 5 -->
<!--     years?* -->
<!-- 4.  *Suppose a large number of people having the same claim -->
<!--     probabilities take out policies at the same time. What is the -->
<!--     proportion would you expect to be in each discount level in the long -->
<!--     run?* -->
<!-- *What would be a suitable model to study the NCD system? As opposed to a -->
<!-- **deterministic model** for which its outcomes are fixed, the outcomes -->
<!-- of the NCD model are uncertain. It turns out that the NCD system can be -->
<!-- studied within the framework of Markov chains, which are examples of -->
<!-- stochastic processes. The use of matrix algebra provides a powerful tool -->
<!-- to understand and analyse the processes.* -->
<!-- *The evolution of the states or levels can be described the random -->
<!-- variables $X_0, X_1,X_2, \ldots$ and probability distributions, where -->
<!-- $X_n$ is the level of the policyholder at time $n$. In this example, the -->
<!-- set of all states called the state space is discrete, which consists of -->
<!-- seven levels, and the time variable is also discrete. This is an example -->
<!-- of a **discrete time, discrete state space stochastic process**.* -->
<!-- ::: -->
<!-- ::: {.example} -->
<!-- **Example 2**. *(**Poisson processes**) Consider the number of claims -->
<!-- that occur up to time $t$ (denoted by $N_t$) from a portfolio of health -->
<!-- insurance policies (or other types of insurance products). Suppose that -->
<!-- the average rate of occurrence of claims per time unit (e.g. day or week -->
<!-- ) is given by $\lambda$.* -->
<!-- *Here are some questions of interest:* -->
<!-- 1.  *On average, 20 claims arrive every day, what is the probability -->
<!--     that more than 100 claims arrive within a week?* -->
<!-- 2.  *What is the expected time until the next claim?* -->
<!-- *In this example, the state space consists of all whole numbers -->
<!-- $\{0, 1, 2, \ldots\}$, while the time variable is continuous. The -->
<!-- process is a **continuous-time stochastic process with discrete state -->
<!-- space**. The model used to model the insurance claims is an example of -->
<!-- **Poisson processes**. The Poisson process is one of the most -->
<!-- widely-used counting processes. Even thought we know that claims occur -->
<!-- at a certain rate, but completely at random. Moreover, the timing -->
<!-- between claims seem to be completely random.* -->
<!-- *Later, we will see that there are several ways to describe this -->
<!-- process. One can focus on the number of claims that occur up to time $t$ -->
<!-- or the times between those claims when they occur. Many important -->
<!-- properties of Poisson processes will be discussed.* -->
<!-- ::: -->
<!-- ::: {.example} -->
<!-- **Example 3**. *(**Markov processes**) Suppose that we observe a total -->
<!-- of $n$ independent lives all aged between $x$ and $x + 1$. For life $i$, -->
<!-- we define the following terms:* -->
<!-- -   *$x+ a_i$ is the age at which observation -->
<!--     begins,$\quad 0 \le a_i < 1$.* -->
<!-- -   *$x+ b_i$ is the age at which observation ends, if life does not -->
<!--     die, $\quad 0 \le b_i < 1$.* -->
<!-- -   *$x+ t_i$ is the age at which observation stops, by death or -->
<!--     censoring.* -->
<!-- -   *$d_i = 1$, if life $i$ dies, otherwise $d_i = 0$, if life $i$ -->
<!--     censored.* -->
<!-- *For example, consider the following mortality data on eight lives all -->
<!-- aged between $70$ and $71$.* -->
<!--    *Life*   *$a_i$*   *$b_i$*   *$d_i$*   *$t_i$* -->
<!--   -------- --------- --------- --------- --------- -->
<!--     *1*       *0*       *1*       *1*     *0.25* -->
<!--     *2*       *0*       *1*       *1*     *0.75* -->
<!--     *3*       *0*       *1*       *0*       *1* -->
<!--     *4*      *0.1*     *0.6*      *1*      *0.5* -->
<!--     *5*      *0.2*     *0.7*      *1*      *0.6* -->
<!--     *6*      *0.2*     *0.4*      *0*      *0.4* -->
<!--     *7*      *0.5*      *1*       *1*     *0.75* -->
<!--     *8*      *0.5*    *0.75*      *0*     *0.75* -->
<!-- *How would one use this dataset to estimate the probability that a life -->
<!-- aged 70 dies before age $70 + t$ or survives to at least age $70 + t$, -->
<!-- for $t \in [0,1)$?* -->
<!-- *In this example, we can represent the process by $\{X_t\}_{t \ge 0}$ -->
<!-- with two possible states (alive or dead). This model is also an example -->
<!-- of a **continuous-time stochastic process with discrete state space**.* -->
<!-- *Here, we illustrate three actuarial applications which can be modelled -->
<!-- by some **stochastic processes**. We should also emphasis that the -->
<!-- outcome of one of the above processes is not fixed or uncertain. The -->
<!-- course will provide important tools and techniques to analyse the -->
<!-- problems with the goal of quantifying the uncertainty in the system.* -->
<!-- ::: -->
<!-- Review of probability theory -->
<!-- ============================ -->
<!-- Random variables -->
<!-- ---------------- -->
<!-- The dynamics of a stochastic process are describes by random variables -->
<!-- and probability distributions. This section provides a brief discussion -->
<!-- of the properties of random variables. -->
<!-- The probability theory is about random variables. Roughly speaking, a -->
<!-- random variable can be regarded as an uncertain, numerical quantity -->
<!-- (i.e. the value in $\mathbb{R}$) whose possible values depend on the -->
<!-- outcomes of a certain random phenomenon. The random variable is usually -->
<!-- denoted by a capital letter $X, Y, \ldots,$ etc.. -->
<!-- More precisely, let $S$ be a sample space. A **random variable** $X$ is -->
<!-- a real-valued function defined on the sample space $S$, -->
<!-- $$X : S \rightarrow \mathbb{R}.$$ Hence, the random variable $X$ is a -->
<!-- function that maps outcomes to real values. -->
<!-- ::: {.example} -->
<!-- **Example 4**. *Two coins are tossed simultaneously and the outcomes are -->
<!-- $HH, HT, TH$ and $TT$. We can associate the outcomes of this experiment -->
<!-- with the set $A = \{1,2,3,4 \}$, where $X(HH) = 1, X(HT) = 2, X(TH) = 3$ -->
<!-- and $X(TT) = 4$. Assume each of the outcomes has an equal probability of -->
<!-- 1/4. Here, we can associate a function $P$ (known as a probability -->
<!-- measure) defined on $S = \{HH, HT, TH, TT \}$ by -->
<!-- $$P(HH) = 1/4,  P(HT) = 1/4,  P(TH) = 1/4,  P(TT) = 1/4.$$* -->
<!-- ::: -->
<!-- A probability measure $P : \mathcal{A} \rightarrow [0,1]$, where -->
<!-- $\mathcal{A}$ is a collection of subsets of $S$, has the following -->
<!-- properties -->
<!-- 1.  $0 \le P(A), \quad A \subset S$. -->
<!-- 2.  $P(S) = 1$. -->
<!-- 3.  If $A_i \cap A_j = \emptyset$ for $i,j = 1,2, \ldots$, and -->
<!--     $i \neq j$ where $A_j \subset S$, then -->
<!--     $$P(\cup^\infty_{i=1} A_i)  = \sum^\infty_{i=1} P(A_i).$$ -->
<!-- Random variables can be discrete or continuous. If the range of a random -->
<!-- variable is finite or countably infinite, then the random variable is a -->
<!-- **discrete random variable**. Otherwise, if its range is an uncountable -->
<!-- set, then it is a **continuous random variable**. -->
<!-- Probability distribution -->
<!-- ------------------------ -->
<!-- The probability distribution of a random variable $X$ is a function -->
<!-- describing all possible values of $X$ and their corresponding -->
<!-- probabilities or the likelihood of obtaining those values of $X$. -->
<!-- Functions that define the probability measure for a discrete or a -->
<!-- continuous random variable are the **probability mass function (pmf)** -->
<!-- and the **probability density function (pdf)**, respectively. -->
<!-- Suppose $X$ is a discrete random variable. Then the function -->
<!-- $$f(x) = P(X = x)$$ that is defined for each $x$ in the range of $X$ is -->
<!-- called the **probability mass function** (p.m.f) of a random variable -->
<!-- $X$. -->
<!-- Suppose $X$ is a continuous random variable with c.d.f $F$ and there -->
<!-- exists a nonnegative, integrable function $f$, -->
<!-- $f: \mathbb{R} \rightarrow [0, \infty)$ such that -->
<!-- $$F(x) = \int_{-\infty}^x f(y)\, dy$$ Then the function $f$ is called -->
<!-- the **probability density function** (p.d.f) of a random variable $X$. -->
<!-- Examples of discrete and continuous random variables {#examples-of-discrete-and-continuous-random-variables .unnumbered} -->
<!-- ---------------------------------------------------- -->
<!-- The main quantities of interest in a portfolio of motor insurance are -->
<!-- the number of claims arriving in a fixed time period and the sizes of -->
<!-- those claims. Clearly, the number of claims can be describe by a -->
<!-- discrete random variable, whose range is finite or countably infinite. -->
<!-- On the other hand, the claim sizes can be describe by a continuous -->
<!-- random variable defined over continuous sample spaces. -->
<!-- ::: {.example} -->
<!-- **Example 5**. *Let $N$ denote the number of claims which arise up to a -->
<!-- given time. The range of all possible values $N$ is -->
<!-- $\mathbf{N} \cup \{0\}$. Here $N$ is an example of discrete random -->
<!-- variable. We could model the number of claims by the Poisson family of -->
<!-- distributions. Recall that a random variable $N$ has a Poisson -->
<!-- distribution with the parameter $\lambda$ if its probability -->
<!-- distribution is given by -->
<!-- $$f(n) = e^{- \lambda} \frac{\lambda^n}{n !}, \quad \text{ for } n = 0,1,\ldots.$$* -->
<!-- *Now suppose further that the number of claims $N$ which arise on a -->
<!-- portfolio in a week has a $\text{Poisson}(\lambda)$ where $\lambda = 5$. -->
<!-- Calculate the following quantities:* -->
<!-- 1.  *$\Pr(N \ge 6)$.* -->
<!-- 2.  *$\mathrm{E}[N]$.* -->
<!-- ::: -->
<!-- 1.  $\Pr(N \ge 6) = 1 - \Pr(N \le 5) = 1 - \sum_{n=0}^5 f(n) = 0.3840393.$ -->
<!-- 2.  Clearly, $\mathrm{E}[N] = \lambda = 5$. -->
<!-- ::: {.example} -->
<!-- **Example 6**. *Let $X$ denote the claim sizes in a given time period. -->
<!-- The range of all possible values $X$ is the set of all non-negative -->
<!-- numbers. Here $X$ is an example of a continuous random variable. -->
<!-- Suitable families of distributions which could be used to modelled claim -->
<!-- sizes are \"fat tails\" distribution. They allow for possibilities of -->
<!-- large claim sizes.* -->
<!-- *Examples of fat-tailed distributions include* -->
<!-- -   *the Pareto distribution,* -->
<!-- -   *the Log-normal distribution,* -->
<!-- -   *the Weibull distribution with shape parameter greater than 0 but -->
<!--     less than 1, and* -->
<!-- -   *the Burr distribution.* -->
<!-- ::: -->
<!-- The course \"SCMA 470 Risk Analysis and Credibility\" provides more -->
<!-- details about the loss distribution. -->
<!-- Conditional probability -->
<!-- ----------------------- -->
<!-- A stochastic process can be defined as a collection or sequence of -->
<!-- random variables. The concept of **conditional probability** plays an -->
<!-- important role to analyse dependency between random variables in the -->
<!-- process. Roughly speaking, conditional probability is the probability of -->
<!-- seeing some event knowing that some other event has actually occurred. -->
<!-- Let $A$ and $B$ be two events (elements of $\mathcal{A}$). The -->
<!-- conditional probability of event $A$ given $B$ denoted by $P(A | B)$ is -->
<!-- defined as $$P(A|B) =  \frac{P(A \cap B)}{P(B)}.$$ Note that -->
<!-- $P(A \cap B)$ is often called the joint probability of $A$ and $B$, and -->
<!-- $P(A)$ and $P(B)$ are often called the marginal probabilities of $A$ and -->
<!-- $B$, respectively. -->
<!-- The events $A$ and $B$ are independent if the occurrence of either one -->
<!-- of the events does not affect the probability of occurrence of the -->
<!-- other. More precisely, the events $A$ and $B$ are independent if -->
<!-- $$P(A \cap B) = P(A)P(B),$$ or equivalently, $$P(A|B) =  P(A).$$ -->
<!-- Law of total probability {#law-of-total-probability .unnumbered} -->
<!-- ------------------------ -->
<!-- Suppose there are three events: $A$, $B$, and $C$. Events $B$ and $C$ -->
<!-- are distinct from each other while event $A$ intersects with both -->
<!-- events. We do not know the probability of event $A$. However, partial -->
<!-- information and dependencies between events can be used to calculate the -->
<!-- probability of event $A$, i.e. we know the probability of event A under -->
<!-- condition B and the probability of event A under condition C. -->
<!-- The total probability rule states that by using the two conditional -->
<!-- probabilities, we can find the probability of event A, which is -->
<!-- $$P(A) = P(A \cap B) + P(A \cap C).$$ In general, suppose -->
<!-- $B_1, B_2, \ldots B_n$ be a collection of events that partition the -->
<!-- sample space. Then for any event $A$, -->
<!-- $$P(A) = \sum_{i = 1}^n   P(A \cap B_i )  = \sum_{i = 1}^n   P(A | B_i ) P(B_i) .$$ -->
<!-- ::: {.example} -->
<!-- **Example 7**. *Suppose in a particular study area, the vaccination rate -->
<!-- for the yearly flu virus is 70%. Suppose of those vaccinated, 10% of the -->
<!-- residents still get the flu that year. Calculate the conditional -->
<!-- probability of someone getting the flu in this area given that the -->
<!-- person was vaccinated.* -->
<!-- ::: -->
<!-- ::: {.example} -->
<!-- **Example 8**. *You are an invester buying shares of a company. You have -->
<!-- discovered that the company is planning to introduce a new project that -->
<!-- is likely to affect the company's stock price. You have determined the -->
<!-- following probabilities:* -->
<!-- -   *There is a 80% probability that the new project will be launched.* -->
<!-- -   *If a company launches the project, there is a 85% probability that -->
<!--     the company's stock price will increase.* -->
<!-- -   *If a company does not launch the project, there is a 30% -->
<!--     probability that the company's stock price will increase.* -->
<!-- ::: -->
<!-- Calculate the probability that the company's stock price will increase. -->
<!-- Conditional distribution and conditional expectation -->
<!-- ---------------------------------------------------- -->
<!-- Let $X$ and $Y$ be two discrete random variables with joint probability -->
<!-- mass function $$f(x,y) = P(X = x, Y = y).$$ If $X$ and $Y$ are -->
<!-- continuous random variables, the joint probability density function -->
<!-- $f (x, y)$ satisfies -->
<!-- $$P( X \le x, Y \le y) = \int_{-\infty}^x \int_{-\infty}^y f(u,v) \, du\, dv.$$ -->
<!-- When no information is given about the value of $Y$, the marginal -->
<!-- probability density function of $X$, $f_X(x)$ is used to calculate the -->
<!-- probabilities of events concerning $X$. However, when the value of $Y$ -->
<!-- is known, to find such probabilities, $f_{X|Y} (x|y)$, the conditional -->
<!-- probability density function of $X$ given that $Y = y$ is used and is -->
<!-- defined as follows: $$f_{X|Y} (x|y)  = \frac{f(x,y)}{f_Y(y)}$$ provided -->
<!-- that $f_Y (y) > 0$. The conditional mass function of $X$ is defined in a -->
<!-- similar manner. $$P(X = x | Y = y) = \frac{P(X = x, Y = y)}{P(X = x)}.$$ -->
<!-- Note also that the conditional probability density function of $X$ given -->
<!-- that $Y = y$ is itself a probability density function, i.e. -->
<!-- $$\int_{-\infty}^\infty f_{X|Y}(x|y)\, dx  =  1.$$ -->
<!-- Note that the conditional probability distribution function of $X$ given -->
<!-- that $Y = y$, the conditional expectation of $X$ given that $Y = y$ can -->
<!-- be as follows: -->
<!-- $$F_{Y|X}(x|y) = P(X \le x | Y = y) = \int_ {-\infty}^x f_{X|Y}(t|y) \, dt$$ -->
<!-- and -->
<!-- $$\mathrm{E}(X|Y = y) =  \int_{-\infty}^{\infty} x  f_{X|Y}(x|y) \, dx,$$ -->
<!-- where $f_Y(y) > 0$. -->
<!-- Note that if $X$ and $Y$ are independent, then $f_{X|Y}$ coincides with -->
<!-- $f_X$ because -->
<!-- $$f_{X|Y}(x|y) = \frac{f(x,y)}{f_Y(y)} =\frac{f_X(x)f_Y(y)}{f_Y(y)} = f_X(x).$$ -->
<!-- Central Limit Theorem -->
<!-- --------------------- -->
<!-- This section introduces the Central Limit Theorem, which is an important -->
<!-- theorem in probability theory. It states that the mean of $n$ -->
<!-- independent and identically distributed random variables has an -->
<!-- approximate normal distribution given a sufficiently large $n$. This -->
<!-- applies to a collection of random variables from any distribution with a -->
<!-- finite mean and variance. In summary we can use the Central Limit -->
<!-- Theorem to extract probabilistic information about the sums of -->
<!-- independent and identical random variables. -->
<!-- Central Limit Theorem {#central-limit-theorem-1 .unnumbered} -->
<!-- --------------------- -->
<!-- Let $X_1, X_2, \ldots$ be a sequence of i.i.d. random variables with a -->
<!-- finite mean $\mathrm{E}[X_i] = \mu$ and finite variance -->
<!-- $\mathrm{Var}[X_i] = \sigma^2$. Let $Z_n$ be the normalised average of -->
<!-- the first $n$ random variables $$\begin{aligned} -->
<!--         Z_n &= \frac{\sum_{i=1}^n X_i/n  - \mu}{\sigma/\sqrt{n}} \\ -->
<!--                &= \frac{X_1 + X_2 + \ldots + X_n  - n\mu}{\sigma \sqrt{n}}. -->
<!--     \end{aligned}$$ Then $Z_n$ converges in distribution to a standard -->
<!-- normal distribution. -->
<!-- Stochastic processes -->
<!-- ==================== -->
<!-- Evolution of a random process is at least partially random, and each run -->
<!-- the process leads to potentially a different outcome. It is of great -->
<!-- interest to understand or model the behaviour of a random process by -->
<!-- describing how different states, represented by random variables $X$'s, -->
<!-- evolve in the system over time. Just as probability theory is considered -->
<!-- as the study of mathematical models of random phenomena, the theory of -->
<!-- stochastic processes plays an important role in the study of -->
<!-- time-dependent random phenomena. Stochastic processes can be used to -->
<!-- represent many different random phenomena from different fields such as -->
<!-- science, engineering, finance, and economics. -->
<!-- A **stochastic process** is a collection of random variables -->
<!-- $\{ X_t : t \in T\}$ defined on a common sample space, where -->
<!-- -   $t$ is a parameter running over some index set $T$, called the -->
<!--     **time domain**. -->
<!-- -   The common sample space of the random variables (the range of -->
<!--     possible values for $X_t$) denoted by $S$ is called the **state -->
<!--     space** of the process. -->
<!-- 1.  The set of random variables may be dependent or need not be -->
<!--     identically distributed. -->
<!-- 2.  Techniques used to study stochastic processes depend on whether the -->
<!--     state space or the index set (the time domain) are discrete or -->
<!--     continuous. -->
<!-- Classification of stochastic processes -->
<!-- -------------------------------------- -->
<!-- Stochastic processes can be classified on the basis of the nature of -->
<!-- their state space and index set. -->
<!-- 1.  **Discrete state space with discrete time changes** : No claims -->
<!--     discount (NCD) policy: A car owner purchases a motor insurance -->
<!--     policy for which the premium charged depends on the claim record. -->
<!--     Let $X_t$ denote the discount status of a policyholder with three -->
<!--     levels of discount, i.e. $S = \{0,1,2\}$ corresponding to three -->
<!--     discount levels of 0%, 20% and 40% and the time set is -->
<!--     $T = \{0,1,2,\ldots\}$. Both time and state space are discrete. -->
<!-- 2.  **Discrete state space with continuous time changes** : In a health -->
<!--     insurance system, an insurance company may classify policyholders as -->
<!--     Healthy, Sick or Dead, i.e. $S = \{H, S, D\}$. The time domain can -->
<!--     be taken as continuous, $T = [0,\infty)$. -->
<!-- 3.  **Continuous state space with continuous time changes** : Let $S_t$ -->
<!--     be the total amount claimed by time $t \in T$ where $T = [0,\infty)$ -->
<!--     and the state space is $\mathbb{R}$. Both time and state space are -->
<!--     continuous. Some continuous time stochastic process taking value in -->
<!--     a continuous state space will be studied in Risk Analysis and -->
<!--     Credibility course. -->
<!-- 4.  **Continuous state space with discrete time changes** : The outcomes -->
<!--     of the above claim process $S_t$ could be recorded continuously, -->
<!--     however, we may choose to model the values only at discrete time, -->
<!--     for e.g. the total claim amounts at the end of each day. This may -->
<!--     due to the limitation of the measurement process (for e.g. expensive -->
<!--     to measure). Hence, the time domain is discrete but the state space -->
<!--     is continuous. -->
<!-- In case that claim amounts are recorded to the nearest baht or in -->
<!-- satang, i.e. discrete state space, we could also approximate or model -->
<!-- the process by using a discrete state space, rather than continuous. -->
<!-- Random walk: an introductory example -->
<!-- ------------------------------------ -->
<!-- One of the simplest examples of a stochastic process is a simple random -->
<!-- walk. Consider a simple model of the price of a stock measured in -->
<!-- satang. For each trading day $n = 0,1,2, \ldots$, the stock price -->
<!-- increases by 1 satang with probability $p$ or decreases by 1 satang with -->
<!-- probability $q = 1-p$. Let $X_n$ denote the stock price at day $n$ and -->
<!-- $X_0 = 100$, i.e. $X_0 =$. This simple model is called a simple random -->
<!-- walk. -->
<!-- In the simple random walk process, time is discrete (as observed at the -->
<!-- end of each day) and the state space is discrete. The stochastic model -->
<!-- has an infinite number of outcomes known as **stochastic realisations or -->
<!-- sample paths**. A **sample path** is then just the sequence of a -->
<!-- particular set of experiments. Graphs of some stochastic realisations of -->
<!-- the simple random walk with $p = 0.5$ and $a = 100$ are shown in Figure -->
<!-- [1]{reference-type="ref" reference="fig:SamplePaths"}. -->
<!-- ![Some stochastic realisations of the simple random -->
<!-- walk](SamplePaths.pdf){#fig:SamplePaths width="4in"} -->
<!-- A complete description of the simple random walk, observed as a -->
<!-- collection of $n$ random variables at time points -->
<!-- $t_1, t_2, \ldots, t_n$ can be specified by the joint distribution of -->
<!-- $X_{t_1}, X_{t_2}, \ldots, X_{t_n}$, i.e. -->
<!-- $$F(x_1, x_2, \ldots, x_n) = \Pr(X_{t_1} \le x_1, X_{t_2} \le x_2, \ldots, X_{t_n} \le x_n).$$ -->
<!-- However, the **multidimensional distribution function cannot be easily -->
<!-- written in a closed form** unless the random variables have a -->
<!-- multivariate normal distribution. In practice, it is more convenient to -->
<!-- deal with some stochastic processes via some simple **intermediary -->
<!-- processes** or under some addition assumptions. -->
<!-- In general, a simple random walk $X_n$ is a discrete-time stochastic -->
<!-- process defined by -->
<!-- -   $X_0 = a$ and -->
<!-- -   for $n \ge1$, -->
<!--     $$X_n = a + \sum_{i=1}^n  Z_i, \text{ where } Z_i = \begin{cases} -->
<!--         1, & \text{ with probability } p  \\ -->
<!--         -1, & \text{ with probability } q =  1- p. -->
<!--      \end{cases}$$ -->
<!-- 1.  When $p = 1/2$, the value of the process increases or decreases -->
<!--     randomly by 1 unit with equal probability. In this case, the process -->
<!--     is known as a **symmetric** random walk. -->
<!-- 2.  The (intermediary) process $\{ Z_i : i \in \mathbb{N}\}$ is a -->
<!--     sequence of independent identically distributed (i.i.d.) random -->
<!--     variables. The process $X_t$ themselves are neither independent nor -->
<!--     identically distributed. This process $Z_i$ is also known as **white -->
<!--     noise process**. -->
<!-- ::: {.example} -->
<!-- **Example 9**. *Explain why the simple random process $X_n$ is neither -->
<!-- independent nor identically distributed.* -->
<!-- ::: -->
<!-- Suppose that $X_0 = 100$. Firstly, we will show that $X_n$ is not -->
<!-- independent. From definition, the process $X_n$ can be written as -->
<!-- $$X_n = X_{n-1} + Z_n.$$ That is, the value of $X_n$ is the previous -->
<!-- value plus a random change of 1 or $-1$. Therefore, the value of the -->
<!-- process depends on the previous value and they are not independent. For -->
<!-- e.g., -->
<!-- $$\Pr(X_2 = 102) > 0 \quad \text{ but } \quad  \Pr(X_2 = 102 | X_1 = 99) = 0.$$ -->
<!-- The process $X_n$ cannot be identically distributed. For e.g. $X_1$ can -->
<!-- take the values of 99 and 101, while $X_2$ can take three different -->
<!-- values of 98, 100 and 102. -->
<!-- ::: {.example} -->
<!-- **Example 10**. *Let $$\begin{aligned} -->
<!--     \mu &= \mathrm{E}[Z_i] \\ -->
<!--     \sigma^2 &= \mathrm{Var}[Z_i] \end{aligned}$$ Calculate the -->
<!-- expectation ($\mu$) and variance ($\sigma^2$) of the random variable -->
<!-- $Z_i$.* -->
<!-- ::: -->
<!-- $$\begin{aligned} -->
<!--     \mu &= \mathrm{E}[Z_i] = 1\cdot p + (-1) \cdot q = p - q.\\\end{aligned}$$ -->
<!-- $$\begin{aligned} -->
<!--     \sigma^2 &= \mathrm{Var}[Z_i] \\ -->
<!--             &=\mathrm{E}[Z_i^2] - (\mathrm{E}[Z_i] )^2 \\ -->
<!--             &= 1 - (p-q)^2  = (p+q)^2 - (p-q)^2\\ -->
<!--             &= 4pq. \end{aligned}$$ -->
<!-- ::: {.example} -->
<!-- **Example 11**. *Calculate the expectation and variance of the process -->
<!-- $X_n$ at time $n$.* -->
<!-- ::: -->
<!-- $$\begin{aligned} -->
<!--      \mathrm{E}[X_n] &=  \mathrm{E}[a + \sum_{i=1}^n Z_i] = a + n\mu.\end{aligned}$$ -->
<!-- $$\begin{aligned} -->
<!--     \mathrm{Var}[X_n]&= \mathrm{Var}[a + \sum_{i=1}^n Z_i]  = n \sigma^2.\end{aligned}$$ -->
<!-- It should be noted that the variance of $X_n$ increases with time. -->
<!-- ::: {.example} -->
<!-- **Example 12**. *For the random process, calculate -->
<!-- $$\Pr(X_2 = 98, X_5 = 99 | X_0 = 100).$$* -->
<!-- ::: -->
<!-- The process $X_n$ must decrease on the first two days, which happens -->
<!-- with probability $(1-p)^2$. Independently, it must then increases on -->
<!-- another two days and decrease on one day (not necessarily in that -->
<!-- order), giving three different possibilities. Each of these has -->
<!-- probability $p^2(1-p)$. So -->
<!-- $$\Pr(X_2 = 98, X_5 = 99 | X_0 = 100) = (1-p)^2 \cdot 3 p^2(1-p) = 3p^2(1-p)^3.$$ -->
<!-- In what follows, we will see that exact calculations are possible for -->
<!-- the simple random walk process. Note also that it is sufficient to -->
<!-- understand the behaviour of the random walk when it starts at $X_0 = 0$. -->
<!-- ::: {.example} -->
<!-- **Example 13**. *For the random process with $X_0 = 0$, $X_{2n}$ is -->
<!-- always even and $X_{2n+1}$ is always odd. Based on the binomial -->
<!-- distribution, show that $$\begin{aligned} -->
<!--     \Pr(X_{2n} = 2m) &= {2n \choose n+m} p^{n+m} q^{n-m}, \quad   -n \le m \le n \\ -->
<!--     \Pr(X_{2n+1} = 2m+1) &= {2n + 1 \choose n+m+1} p^{n+m+1} q^{n-m}, \quad   -n-1 \le m \le n.\end{aligned}$$* -->
<!-- ::: -->
<!-- Let $A$ denote the number of $+1$ and $B$ denote the number of $-1$. -->
<!-- Then $A + B = 2n$ and $X_{2n} = A - B$ (i.e. the position at time $2n$). -->
<!-- Hence, $$\begin{aligned} -->
<!--     \Pr(X_{2n} = 2m) &= \Pr( A - B = 2m) \\ -->
<!--     &= \Pr( A - (2n - A) = 2m)  =   \Pr( 2A - 2n  = 2m) =   \Pr( A   = m + n)\\ -->
<!--     &= {2n \choose n+m} p^{n+m} q^{2n-(n+m)}, \quad   -n \le m \le n \\ -->
<!--     &= {2n \choose n+m} p^{n+m} q^{n-m}, \quad   -n \le m \le n.\end{aligned}$$ -->
<!-- # Discrete-time Markov chains -->
<!-- # Discrete-time Markov chains -->
<!-- Recall the simple random walk model of the price of the stock. Suppose -->
<!-- the stock price for the first four days are -->
<!-- $$(X_0, X_1, X_2, X_3) = (100, 99, 98, 99).$$ Based on this past -->
<!-- information, what can we say about the price at day 4, $X_4$? Although, -->
<!-- we completely know the whole past price history, the only information -->
<!-- relevant for predicting their future price is the price on the previous -->
<!-- day, i.e. $X_3$. This means that -->
<!-- $$\Pr(X_{4} = j | X_0 = 100, X_{1} = 99,  X_2 = 98 ,  X_3 = 99) = \Pr(X_{4} = j | X_3 = 99).$$ -->
<!-- Given the current price $X_3$, the price $X_4$ at day 4 is independent -->
<!-- of the history prices $X_0, X_1, X_2$. The sequence of stock prices -->
<!-- $X_0, X_1, \ldots, X_n$ is an example of a **Markov chain**. -->
<!-- A Markov process is a special type of stochastic processes with the -->
<!-- property that the future evolution of the process depends only on its -->
<!-- current state and not on its past history. That is given the value of -->
<!-- $X_t$, the values of $X_s$ for $s > t$ do not depend on the values of -->
<!-- $X_u$ for $u < t$. This property is called the **Markov property**. -->
<!-- A **discrete-time Markov chain** is a discrete-time stochastic process -->
<!-- that satisfies the Markov property: -->
<!-- $$\Pr(X_{n+1} = j | X_n = i, X_{n-1} = i_{n-1}, \ldots, X_0 = i_{0}) = \Pr(X_{n+1} = j | X_n = i),$$ -->
<!-- for all time points $n$ and all states $i_0, i_1, \ldots, i_{n-1},i,j$. -->
<!-- It is convenient to assume that the state space of the Markov chain is a -->
<!-- subset of non-negative integers, i.e. $S  \subseteq \{0, 1,  \ldots \}$. -->
<!-- ::: {.example} -->
<!-- **Example 1**. *A process with independent increments has the Markov -->
<!-- property.* -->
<!-- ::: -->
<!-- Recall the following definitions. An increment of a process is the -->
<!-- amount by which its value changes over a period of time, for e.g. -->
<!-- $X_{t +u} - X_t$ where $u > 0$. -->
<!-- A process $X_t$ is said to have independent increments if for all $t$ -->
<!-- and every $u > 0$, the increment $X_{t +u} - X_t$ is independent of all -->
<!-- the past of the process $\{X_s : 0 \le s \le t \}$). -->
<!-- In order to show that a process with independent increments has the -->
<!-- Markov property, we proceed as follows: $$\begin{aligned} -->
<!-- \Pr(X_t \in A | X_{s_1} = x_1, X_{s_2} = x_2, \ldots, X_{s} = x) -->
<!-- &= \Pr(X_t - X_s + x \in A | X_{s_1} = x_1, X_{s_2} = x_2, \ldots, X_{s} = x) \\ -->
<!-- &= \Pr(X_t - X_s + x \in A |  X_{s} = x)  \text{(by independence of the past)} \\ -->
<!-- &= \Pr(X_t  \in A |  X_{s} = x).\end{aligned}$$ -->
<!-- The random walk process has the Markov property. -->
<!-- One-step transition probabilities -->
<!-- --------------------------------- -->
<!-- The conditional probability that $X_{n+1}$ is in state $j$ given that -->
<!-- $X_n$ is in state $i$ is called **one-step transition probability** and -->
<!-- is denoted by $$\Pr(X_{n+1} = j | X_n = i) = p_{ij}^{n,n+1}.$$ Note that -->
<!-- the transition probabilities depend not only on the current and future -->
<!-- states, **but also on the time of transition $n$**. -->
<!-- If the transition probabilities $p_{ij}^{n,n+1}$ in a Markov chain do -->
<!-- not depend on time $n$, the Markov chain is said to be -->
<!-- **time-homogeneous or stationary or simply homogeneous**. Then -->
<!-- $$p_{ij}^{n,n+1} = \Pr(X_{n+1} = j | X_n = i)  = \Pr(X_{1} = j | X_0 = i)  = p_{ij}.$$ -->
<!-- Otherwise, it is said to be **nonstationary** or **nonhomogeneous**. -->
<!-- Unless stated otherwise, it shall be assumed that the Markov chain is -->
<!-- stationary. The matrix $P$ whose elements are $p_{ij}$ is called the -->
<!-- **transition probability matrix** of the process. $$P = \begin{bmatrix} -->
<!--     p_{11} & p_{12} & p_{13} & \dots   \\ -->
<!--     p_{21} & p_{22} & p_{23} & \dots   \\ -->
<!--     p_{31} & p_{32} & p_{33} & \dots   \\ -->
<!--     \vdots & \vdots & \vdots  & \vdots \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix}.$$ Note that the elements of the matrix $P$ satisfies the -->
<!-- following properties: -->
<!-- $$\ 0 \le p_{ij} \le 1, \quad \text{ and } \quad \sum_{j \in S}p_{ij} = 1,$$ -->
<!-- for all $i,j \in S.$ A matrix that satisfies these properties is called -->
<!-- a **stochastic matrix**. -->
<!-- ::: {#NCD .example} -->
<!-- **Example 2**. *No claims discount (NCD) policy: Let $X_n$ be the -->
<!-- discount status of a policyholder at time $n$. There are three levels of -->
<!-- discount, i.e. $S = \{0,1,2\}$ corresponding to three discount levels of -->
<!-- 0, 20% and 40%. The following rules are assumed:* -->
<!-- -   *For a claim-free year, the policyholder moves up a level or remains -->
<!--     in state 2 (the maximum discount state).* -->
<!-- -   *If there is at least one claim, the policyholder moves down one -->
<!--     level or remains in state 0.* -->
<!-- *Suppose also that the probability of a claim-free year is $p$ and is -->
<!-- independent of $n$. The transition probability matrix is given by -->
<!-- $$P = \begin{bmatrix} -->
<!--     1- p & p & 0    \\ -->
<!--     1-p & 0 & p   \\ -->
<!--     0 & 1-p & p    \\ -->
<!--     %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix}.$$ The transition diagram is illustrated in the following -->
<!-- figure.* -->
<!-- *The following questions are of interest.* -->
<!-- 1.  *What is the probability -->
<!--     $$\Pr(X_0 = i_0, X_1 = i_1, X_2 = i_2, \ldots, X_n = i_n)?$$* -->
<!-- 2.  *What is the probability -->
<!--     $$\Pr(X_{n+1} = i_{n+1}, X_{n+2} = i_{n+2},  \ldots, X_{n+m} = i_{n+m}| X_n = i_n, X_{n-1} = i_{n-1}, \ldots, X_0 = i_{0})?$$* -->
<!-- 3.  *What is the probability of transferring from state $i$ to state $j$ -->
<!--     in $n$ steps $$\Pr(X_{m+n} = j | X_m = i )?$$* -->
<!-- 4.  *What is the long-term behavior of the Makov chain, i.e. -->
<!--     $\lim_{n \rightarrow \infty} \Pr(X_n = j), j = 0,1,2$ given that -->
<!--     $\Pr(X_0 = 0)$.* -->
<!-- ::: -->
<!-- Later we will apply matrix algebra to compute these types of -->
<!-- probabilities and long-term probabilities. -->
<!-- ::: {#healthInsurance .example} -->
<!-- **Example 3**. *In a health insurance system, at the end of each day an -->
<!-- insurance company classifies policyholders as Healthy, Sick or Dead, -->
<!-- i.e. $S = \{H, S, D\}$. The following transition matrix $P$ for a -->
<!-- healthy-sick-dead model is given by $$P = \begin{bmatrix} -->
<!--     p_{11} & p_{12} & p_{13}    \\ -->
<!--     p_{21} & p_{22} & p_{23}   \\ -->
<!--    0 & 0 & 1   \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix}.$$* -->
<!-- ::: -->
<!-- The transition diagram is shown below. -->
<!-- It turns out that the probabilistic description of the Markov chain is -->
<!-- completely determined by its transition probability matrix and its -->
<!-- initial probability distribution $X_0$ at time 0. -->
<!-- ::: {.example} -->
<!-- **Example 4**. *By using the definition of conditional probabilities, -->
<!-- show that -->
<!-- $$\Pr(X_0 = i_0, X_1 = i_1, X_2 = i_2, \ldots, X_n = i_n) = \mu_{i_0}\, p_{i_0i_1} \cdots \, p_{i_{n-2} i_{n-1}}\, p_{i_{n-1} i_n},$$ -->
<!-- where $\boldsymbol{\mu} = \boldsymbol{\mu}^{(0)}$ is the distribution of -->
<!-- initial random variable $X_0$, i.e. $\mu_i = \Pr(X_0 = i)$ (the -->
<!-- probability mass function of $X_0$.)* -->
<!-- ::: -->
<!-- $$\begin{aligned} -->
<!--  &\Pr(X_0 = i_0, X_1 = i_1, \ldots, X_n = i_n) \\ -->
<!--   &= \Pr(X_0 = i_0, X_1 = i_1, \ldots, X_{n-1} = i_{n-1}) \cdot \Pr(X_n = i_n | X_0 = i_0, X_1 = i_1, \ldots, X_{n-1} = i_{n-1})\\ -->
<!--   &= \Pr(X_0 = i_0, X_1 = i_1, \ldots, X_{n-1} = i_{n-1}) \cdot \Pr(X_n = i_n |  X_{n-1} = i_{n-1})\\ -->
<!--   &=  \Pr(X_0 = i_0, X_1 = i_1, \ldots, X_{n-1} = i_{n-1}) \cdot p_{i_{n-1} i_n} \\ -->
<!--   &\quad \vdots \\ -->
<!--   &= \mu_{i_0}\, p_{i_0i_1} \cdots \, p_{i_{n-2} i_{n-1}}\, p_{i_{n-1} i_n}. -->
<!--   \end{aligned}$$ -->
<!-- ::: {#MKProperty1 .example} -->
<!-- **Example 5**. *Show that $$\begin{aligned} -->
<!-- \Pr(X_{n+1} &= i_{n+1}, X_{n+2} = i_{n+2},  \ldots, X_{n+m} = i_{n+m}| X_n = i_n, X_{n-1} = i_{n-1}, \ldots, X_0 = i_{0}) \\ -->
<!--             &= \Pr(X_{n+1} = i_{n+1}, X_{n+2} = i_{n+2},  \ldots, X_{n+m} = i_{n+m}| X_n = i_n) \\ -->
<!--             &=  p_{i_{n} i_{n+1}} \cdots \, p_{i_{n+m-2} i_{n+m-1}}\, p_{i_{n+m-1} i_{n+m}}.\end{aligned}$$* -->
<!-- ::: -->
<!-- $$\begin{aligned} -->
<!-- &\Pr(X_{n+1} = i_{n+1}, X_{n+2} = i_{n+2},  \ldots, X_{n+m} = i_{n+m}| X_n = i_n, X_{n-1} = i_{n-1}, \ldots, X_0 = i_{0}) \\ -->
<!-- &= \Pr(X_{n+1} = i_{n+1}, X_{n+2} = i_{n+2},  \ldots, X_{n+m} = i_{n+m}| X_n = i_n) \\ -->
<!-- &= \Pr( X_{n+2} = i_{n+2},  \ldots, X_{n+m} = i_{n+m}| X_{n+1} = i_{n+1}, X_n = i_n)  \cdot \Pr(X_{n+1} = i_{n+1} |  X_n = i_n ) \\ -->
<!-- &= \Pr( X_{n+2} = i_{n+2},  \ldots, X_{n+m} = i_{n+m}| X_{n+1} = i_{n+1})  \cdot p_{i_n i_{n+1}} \\ -->
<!-- &\quad \vdots \\ -->
<!-- &= \Pr( X_{n+m} = i_{n+m}| X_{n+m-1} = i_{n+m-1}) \cdots \Pr( X_{n+2} = i_{n+2}| X_{n + 1} = i_{n + 1}) \cdot p_{i_n i_{n+1}} \\ -->
<!-- &= p_{i_{n+m-1} i_{n+m}} \cdot p_{i_{n+m-2} i_{n+m-1}}   \cdots p_{i_n i_{n+1}} .\end{aligned}$$ -->
<!-- More general probabilities of the possible realisations of the process -->
<!-- can be calculated by summing the probabilities of elementary elements of -->
<!-- these forms. -->
<!-- ::: {.example} -->
<!-- **Example 6**. *For the NCD system defined on the state space -->
<!-- $S = \{0,1,2\}$ as given in -->
<!-- Example [Example 2](#NCD){reference-type="ref" reference="NCD"}, suppose -->
<!-- that the probability of a claim-free year $p = 3/4$, and the -->
<!-- distribution of the initial discount rate -->
<!-- $\boldsymbol{\mu}  = (0.5,0.3,0.2)$. Find the following:* -->
<!-- ::: -->
<!-- 1.  $\Pr(X_0 = 2, X_1 = 1, X_2 = 0).$ -->
<!-- 2.  $\Pr( X_1 = 1, X_2 = 0 | X_0 = 2).$ -->
<!-- 3.  $\Pr(X_{10} = 2, X_{11} = 1, X_{12} = 0).$ -->
<!-- 4.  $\Pr( X_{11} = 1, X_{12} = 0 | X_{10} = 2).$ -->
<!-- The corresponding transition matrix is $$P = \begin{bmatrix} -->
<!--     1/4 & 3/4 & 0    \\ -->
<!--     1/4 & 0 & 3/4   \\ -->
<!--     0 & 1/4 & 3/4    \\ -->
<!--     %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix}.$$ -->
<!-- 1.  Denote $\boldsymbol{\mu}  = (\mu_1, \mu_2, \mu_3) =  (0.5,0.3,0.2)$ -->
<!--     $$\begin{aligned} -->
<!--         \Pr(X_0 = 2, X_1 = 1, X_2 = 0) &=  \Pr(X_0 = 2, X_1 = 1) \cdot \Pr(X_2 = 0 | X_0 = 2, X_1 = 1) \\ -->
<!--         &= \Pr(X_0 = 2, X_1 = 1) \cdot \Pr(X_2 = 0 | X_1 = 1) \quad \text{(by Markov property)}\\ -->
<!--         &= \Pr(X_0 = 2) \cdot \Pr(X_1 = 1 | X_0 = 2 ) \cdot  \Pr(X_2 = 0 | X_1 = 1) \quad  \text{(again by Markov property)}\\ -->
<!--         &= \mu_3 p_{32} p_{21} = 0.2\cdot(1/4)\cdot(1/4) = 1/80.\end{aligned}$$ -->
<!--     Alternatively, it follows from -->
<!--     Example [Example 5](#MKProperty1){reference-type="ref" -->
<!--     reference="MKProperty1"} that, -->
<!--     $$\Pr(X_0 = 2, X_1 = 1, X_2 = 0) = \mu_3 p_{32} p_{21} = 0.2\cdot(1/4)\cdot(1/4) = 1/80.$$ -->
<!-- 2.  $$\begin{aligned} -->
<!--     \Pr( X_1 = 1, X_2 = 0 | X_0 = 2) -->
<!--     &=   \Pr( X_1 = 1 | X_0 = 2)  \cdot \Pr( X_2 = 0 | X_1 = 1, X_0 = 2) \\ -->
<!--     &=   \Pr( X_1 = 1 | X_0 = 2) \cdot \Pr( X_2 = 0 | X_1 = 1) \quad \text{(by Markov property)}\\ -->
<!--     &= p_{32} p_{21} = (1/4)\cdot(1/4) = 1/16.\end{aligned}$$ -->
<!-- 3.  Following conditional probability, the Markov property, and -->
<!--     time-homogeneity (to be discussed later) results in -->
<!--     $$\begin{aligned} -->
<!--         \Pr(X_{10} = 2, X_{11} = 1, X_{12} = 0) &=  \Pr(X_{10} = 2)  \cdot \Pr(X_{11} = 1, X_{12} = 0 | X_{10} = 2) \\ -->
<!--          &=  \Pr(X_{10} = 2)  \cdot \Pr(X_{11} = 1 | X_{10} = 2) \cdot \Pr( X_{12} = 0 | X_{10} = 2, X_{11} = 1)  \\ -->
<!--          &=  \Pr(X_{10} = 2)  \cdot \Pr(X_{11} = 1 | X_{10} = 2) \cdot \Pr( X_{12} = 0 |  X_{11} = 1)  \\ -->
<!--          &= \Pr(X_{10} = 2)  \cdot \Pr(X_{1} = 1 | X_{0} = 2) \cdot \Pr( X_{1} = 0 |  X_{0} = 1) \\ -->
<!--           &= \Pr(X_{10} = 2)  \cdot p_{32} p_{21} = 0.6922\cdot (1/4)\cdot(1/4) = 0.0433.  \\\end{aligned}$$ -->
<!--     Later we will show that -->
<!--     $\Pr(X_{10} = 2) = (\boldsymbol{\mu} P^{10})_3 = 0.6922$ (here -->
<!--     $(\boldsymbol{\mu} P^{10})_i$ denotes the $i$-th entry of the vector -->
<!--     $\boldsymbol{\mu} P^{10}$. -->
<!-- 4.  From conditional probability, the Markov property, and -->
<!--     time-homogeneity, it follows that $$\begin{aligned} -->
<!--     \Pr( X_{11} = 1, X_{12} = 0 | X_{10} = 2) -->
<!--     &=   \Pr( X_{11} = 1 | X_{10} = 2)  \cdot \Pr( X_{12} = 0 | X_{11} = 1, X_{10} = 2) \\ -->
<!--     &=   \Pr( X_{11} = 1 | X_{10} = 2) \cdot \Pr( X_{12} = 0 | X_{11} = 1) \quad \text{(by Markov property)}\\ -->
<!--     &=   \Pr( X_{1} = 1 | X_{0} = 2) \cdot \Pr( X_{1} = 0 | X_{0} = 1) \quad \text{(by time-homogeneity)}\\ -->
<!--     &= p_{32} p_{21} = (1/4)\cdot(1/4) = 1/16.\end{aligned}$$ -->
<!-- The Chapman-Kolmogorov equations -->
<!-- ================================ -->
<!-- The Chapman-Kolmogorov equations and $n$-step transition probabilities -->
<!-- ---------------------------------------------------------------------- -->
<!-- The $n$-step transition probability denoted by $p^{(n)}_{ij}$ is the -->
<!-- probability that the process goes from state $i$ to state $j$ in $n$ -->
<!-- transitions, i.e. $$p^{(n)}_{ij} = \Pr(X_{m+n} = j | X_m = i ).$$ Note -->
<!-- that for homogeneous process, the left hand side of the above equation -->
<!-- does not depend on $m$. Suppose that the transition from state $i$ at -->
<!-- time $m$ to state $j$ at time $m+n$ (i.e. in $n$ steps), going via state -->
<!-- $k$ in $l$ steps. One needs to examine all possible paths (from $i$ to -->
<!-- $k$ and then $k$ to $j$) and hence the $n$-step transition probability -->
<!-- $p^{(n)}_{ij}$ can be expressed as the sum of the product of the -->
<!-- transition probabilities $p^{(l)}_{ik} \, p^{(n-l)}_{kj}$. -->
<!-- $$\begin{aligned} -->
<!--  \label{Chapman} -->
<!--     p^{(n)}_{ij} = \sum_{k \in S}p^{(l)}_{ik} p^{(n-l)}_{kj}, \quad 0 < l < n  \end{aligned}$$ -->
<!-- To derive [\[Chapman\]](#Chapman){reference-type="eqref" -->
<!-- reference="Chapman"}, we proceed as follows: -->
<!-- $$\begin{aligned} -->
<!--     p^{(n)}_{ij} &= \Pr(X_n = j | X_0 = i) \\ -->
<!--     &= \sum_{k \in S} \Pr(X_n = j , X_l = k | X_0 = i)    \\ -->
<!--     &= \sum_{k \in S} \Pr(X_n = j | X_l = k , X_0 = i)  \cdot   \Pr(X_l = k |  X_0 = i) \\ -->
<!--     &= \sum_{k \in S} \Pr(X_n = j | X_l = k )  \cdot   \Pr(X_l = k |  X_0 = i) \\ -->
<!--     &= \sum_{k \in S}p^{(n-l)}_{kj} p^{(l)}_{ik} , = \sum_{k \in S}p^{(l)}_{ik} p^{(n-l)}_{kj}, \quad 0 < l < n . \end{aligned}$$ -->
<!-- This result is known as the Chapman-Kolmogorov equation. This relation -->
<!-- can be expressed in terms of matrix multiplication as -->
<!-- $$P^n  = P^l P^{n-l}.$$ The $n$-**step transition probabilities** -->
<!-- $p^{(n)}_{ij}$ are the $ij$ elements of $P^n$. -->
<!-- ::: {.example} -->
<!-- **Example 7**. *For the NCD system given in -->
<!-- Example [Example 2](#NCD){reference-type="ref" reference="NCD"}, suppose -->
<!-- that $p = 3/4$, the probability of a claim-free year and the initial -->
<!-- discount level of a policyholder is 1 (with 20% discount).* -->
<!-- 1.  *Calculate the probability of starting with a discount level of 20% -->
<!--     and ending up 3 years later at the same level.* -->
<!-- 2.  *Calculate the policyholder's expected level of discount after 3 -->
<!--     years.* -->
<!-- ::: -->
<!-- The transition matrix is $$P = \begin{bmatrix} -->
<!--     1/4 & 3/4 & 0    \\ -->
<!--     1/4 & 0 & 3/4   \\ -->
<!--     0 & 1/4 & 3/4    \\ -->
<!--     %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix},$$ and $$P^3 = \begin{bmatrix} -->
<!--     7/64 & 21/64 & 9/16    \\ -->
<!--     7/64 & 3/16 & 45/64   \\ -->
<!--     1/16 & 15/64 & 45/64    \\ -->
<!--     %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix}.$$ -->
<!-- 1.  The probability of starting with a discount level of 20% and ending -->
<!--     up 3 years later at the same level is equal to -->
<!--     $p_{11}^{(3)} = 3/16,$ which is the element in the second row and -->
<!--     second column of the matrix $P^3$ (not to be confused with the -->
<!--     indices used) . -->
<!-- 2.  The policy's expected level of discount after 3 years is -->
<!--     $$\begin{aligned} -->
<!--     \mathrm{E}[X_3 | X_0 = 1] &= \sum_{j=0}^{2} j \cdot  \Pr(X_3 = j | X_0 = 1) \\ -->
<!--     &= 0 \cdot (7/64) + 1 \cdot (3/16) + 2 \cdot (45/64)   \\ -->
<!--     &= 51/32 = 1.59375.\end{aligned}$$ -->
<!-- Distribution of $X_n$ -->
<!-- --------------------- -->
<!-- Let $\boldsymbol{\mu}^{(n)}$ be the vector of probability mass function -->
<!-- of $X_n$, i.e. $$\boldsymbol{\mu}^{(n)} = (\mu_1, \mu_2, \ldots ),$$ -->
<!-- where $\mu_i = \Pr(X_n = i)$. It follows that -->
<!-- $$\boldsymbol{\mu}^{(n+1)} = \boldsymbol{\mu}^{(n)} P$$ and, in general, -->
<!-- $$\boldsymbol{\mu}^{(n+m)} = \boldsymbol{\mu}^{(n)} P^m.$$ -->
<!-- ::: {.example} -->
<!-- **Example 8**. -->
<!-- 1.  *Show that -->
<!--     $$Pr(X_1 = i) = \sum_{k \in S} \mu_k p_{ki} = ( \boldsymbol{\mu} P)_i,$$ -->
<!--     which is the $i$th element of the vector $\boldsymbol{\mu} P.$ Here -->
<!--     $\boldsymbol{\mu} = \boldsymbol{\mu}^{0}$ is the distribution of -->
<!--     initial random variable $X_0$ with $\mu_i = \Pr(X_0 = i)$.* -->
<!-- 2.  *In general, show that the distribution of $X_n$ is given by -->
<!--     $$Pr(X_n = i) =  ( \boldsymbol{\mu} P^n)_i.$$* -->
<!-- ::: -->
<!-- 1.  $$\begin{aligned} -->
<!--         \Pr(X_1 = i) &= \sum_{k \in S} \Pr(X_1 = i | X_0 = k) \cdot \Pr(X_0 = k) \\ -->
<!--         &=  \sum_{k \in S} \mu_k \cdot p_{ki} \\ -->
<!--         &= ( \boldsymbol{\mu} P)_i.\end{aligned}$$ -->
<!-- ::: {#weather .example} -->
<!-- **Example 9**. *The simple weather pattern can be classified into three -->
<!-- types including rainy ($R$), cloudy ($C$) and sunny ($S$). The weather -->
<!-- is observed daily. The following information is provided.* -->
<!-- -   *On any given rainy day, the probability that it will rain the next -->
<!--     day is 0.7; the probability that it will be cloudy the next day -->
<!--     0.2.* -->
<!-- -   *On any given cloudy day, the probability that it will rain the next -->
<!--     day is 0.75; the probability that it will be sunny the next day -->
<!--     0.1.* -->
<!-- -   *On any given sunny day, the probability that it will rain the next -->
<!--     day is 0.2; the probability that it will be sunny the next day 0.4.* -->
<!-- *The weather forecast for tomorrow shows that there is a 40% chance of -->
<!-- rain and a 60% chance of cloudy. Find the probability that it will sunny -->
<!-- 2 days later.* -->
<!-- ::: -->
<!-- As the ordered state of the chain is $R, C, S$, the initial distribution -->
<!-- is $\boldsymbol{\mu} = (0.4, 0.6, 0)$. The transition matrix $P$ is -->
<!-- given by $$P = \begin{bmatrix} -->
<!--     0.7 & 0.2 & 0.1    \\ -->
<!--     0.75 & 0.15 & 0.1   \\ -->
<!--     0.2 & 0.4 & 0.4   \\ -->
<!--     %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix},$$ and $$P^2 = \begin{bmatrix} -->
<!--     0.66 & 0.21 & 0.13    \\ -->
<!--     0.6575 & 0.2125 & 0.13   \\ -->
<!--     0.52 & 0.26 & 0.22   \\ -->
<!--     %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix}.$$ This gives -->
<!-- $$\boldsymbol{\mu} \cdot P^2 = (0.6585,0.2115, 0.13).$$ Hence, the -->
<!-- desired probability of sunny is -->
<!-- $$\Pr(X_2 = S) =( \boldsymbol{\mu} \cdot P^2 )_S  = (\boldsymbol{\mu} \cdot P^2 )_3 = 0.13.$$ -->
<!-- Joint Distribution -->
<!-- ------------------ -->
<!-- Let $X_0, X_1, \ldots$ be a Markov chain with transition matrix $P$ and -->
<!-- initial distribution $\boldsymbol{\mu}.$ For all -->
<!-- $0 \le n_1 \le n_2 < \cdots < n_{k-1} < n_k$ and states -->
<!-- $i_1, i_2, \ldots , i_{k-1}, i_k,$ -->
<!-- $$P(X_{n_1} = i_1, X_{n_2} = i_2,\ldots, X_{n_k} = i_{k1}1, X_{n_k} = i_k) -->
<!-- = (\boldsymbol{\mu} P^{n_1} )_{i_1} (P^{n_2 - n_1} )_{i_1i_2} \cdots (P^{n_k - n_{k -1}} )_{i_{k-1}i_k}.$$ -->
<!-- From the above result, the joint probability is obtained from just the -->
<!-- initial distribution $\boldsymbol{\mu}$ and the transition matrix $P$. -->
<!-- ::: {.example} -->
<!-- **Example 10**. *In Example [Example 9](#weather){reference-type="ref" -->
<!-- reference="weather"}, on Sunday, the chances of rain, cloudy and sunny -->
<!-- have the same probabilities. Find the probability that it will be sunny -->
<!-- on the following Wednesday and Friday, and cloudy on Saturday.* -->
<!-- ::: -->
<!-- We are given that $\boldsymbol{\mu} = (1/3, 1/3, 1/3).$ From -->
<!-- $$P^3 = \begin{bmatrix} -->
<!--     0.645500 & 0.215500 & 0.139   \\ -->
<!--     0.645625 & 0.215375 & 0.139  \\ -->
<!--     0.603000 & 0.231000 & 0.166   \\ -->
<!--     %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix},$$ the required probability is $$\begin{aligned} -->
<!--  \Pr(X_3 = S, X_5 = S, X_6 = C) &= (\boldsymbol{\mu}  \cdot P^3)_S \cdot  P^{2}_{SS} \cdot P_{SC} \\ -->
<!-- &= (\boldsymbol{\mu}  \cdot P^3)_3 \cdot P^{2}_{33} \cdot P_{32}  \\ -->
<!-- &= 0.148 \cdot  0.22 \cdot 0.4 = 0.013024.\end{aligned}$$ -->
<!-- Random walk with absorbing and reflecting barrier(s) -->
<!-- ---------------------------------------------------- -->
<!-- ::: {.example} -->
<!-- **Example 11**. *A one-dimensional random walk $\{X_n\}$ is defined on a -->
<!-- finite or infinite subset of integers in which the process in state $i$ -->
<!-- can either stay in $i$ or move to its neighbouring states $i -1$ and -->
<!-- $i+1$. Suppose that given that $X_n = i$ at time $n$,* -->
<!-- -   *the probability of moving to state $i+1$ is $p_i$,* -->
<!-- -   *the probability of remaining in state $i$ is $r_i$, and* -->
<!-- -   *the probability of moving to state $i-1$ is $q_i$,* -->
<!-- *where $p_i + q_i + r_i = 1$ for all $i$.* -->
<!-- 1.  *Write down the transition matrix.* -->
<!-- 2.  *Show that the random walk has Markov property.* -->
<!-- ::: -->
<!-- 1.  The transition diagram and the transition matrix are infinite: -->
<!--     $$P = \begin{bmatrix} -->
<!--         \ddots & \cdots & \cdots & \cdots & \cdots & \cdots  & \cdots    \\ -->
<!--         \cdots & q_{-1} & r_{-1}  & p_{-1}  & \cdots & \cdots  & \cdots  \\ -->
<!--         \cdots & \cdots & q_{0} & r_{0}  & p_{0}  & \cdots   & \cdots  \\ -->
<!--         \cdots & \cdots & \cdots & q_{1} & r_{1}  & p_{1}    & \cdots    \\ -->
<!--         \cdots & \cdots & \cdots & \cdots & \cdots  & \cdots  & \ddots   \\ -->
<!--      %0 & 0 & 0 & 1/2 & 1/2     \\ -->
<!--         %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!--     \end{bmatrix}.$$ -->
<!-- 2.  Clearly, the Markov property holds because $$\begin{aligned} -->
<!--         &\Pr(X_{n+1} = k | X_n = i, X_{n-1} = i_{n-1}, \ldots, X_0 = i_0)  \\ -->
<!--         &= \Pr(X_{n+1} = k | X_n = i) = -->
<!--         \begin{cases} -->
<!--                    p_i,              & k = i+1\\ -->
<!--                    r_i,              & k = i\\ -->
<!--                    q_i,               & k = i-1\\ -->
<!--                    0,       & \text{otherwise}. -->
<!--                \end{cases}\end{aligned}$$ -->
<!-- ::: {#simpleRW .example} -->
<!-- **Example 12**. *The random walk can be used to model the fortune of a -->
<!-- gambler. The gambler bets per game and the probability of winning is $p$ -->
<!-- and the probability of losing is $q$ where $p + q = 1$. In addition, the -->
<!-- gambler is ruined (or goes broke) if he reaches state 0, and also stops -->
<!-- the game if he reaches state $N$. Therefore, the state space is -->
<!-- $S = \{0, 1, \ldots, N\}$. Note that -->
<!-- $$p_{00} = 1 \text { and } p_{NN} = 1.$$ The states $0$ and $N$ are -->
<!-- referred to as **absorbing boundaries (absorbing states)** and the -->
<!-- remaining states $1,2,\ldots,N-1$ are **transient**. Roughly speaking, -->
<!-- if a state is known as transient if there is a possibility of leaving -->
<!-- the state and never returning.* -->
<!-- ::: -->
<!-- The transition diagram and the transition matrix of the simple random -->
<!-- walk with absorbing boundaries (states) are given as follows: -->
<!-- $$P = \begin{bmatrix} -->
<!--     1& 0 & 0 & 0& \cdots & 0& 0  & 0    \\ -->
<!--     q & 0 & p  & 0& \cdots & 0& 0  & 0  \\ -->
<!--     0 & q & 0 & p  & \cdots & 0& 0 & 0   \\ -->
<!--     \vdots & \vdots & \vdots & \vdots &  \ddots & \vdots    & \vdots & \vdots   \\ -->
<!--     0 & 0 & 0 & 0  & \cdots & q & 0 & p   \\ -->
<!--     0 & 0 & 0 & 0  & \cdots & 0 & 0 & 1   \\ -->
<!--  %0 & 0 & 0 & 1/2 & 1/2     \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix}.$$ -->
<!-- 1.  In general, a state $i$ is called **absorbing** if $p_{ii} = 1$ -->
<!-- 2.  The simple random walk as given in Example -->
<!--     [Example 12](#simpleRW){reference-type="ref" reference="simpleRW"} -->
<!--     can be modified so that whenever the process is in state 0 (or state -->
<!--     $N$), -->
<!--     -   the probability of remaining in state 0 is $\alpha$, and -->
<!--     -   the probability of moving to state 1 is $1 - \alpha$. -->
<!--     In this case, the state 0 is referred to as a **reflecting barrier** -->
<!--     for the chain. The process might be used to model the fortune of an -->
<!--     individual when negative fortune is reset to zero. -->
<!-- An example of nonhomogeneous Markov chain -->
<!-- ----------------------------------------- -->
<!-- In this section, we give an example of a discrete-time nonhomogeneous -->
<!-- Markov chain. Again, without stated otherwise, we shall assume that the -->
<!-- discrete-time Markov chains are homogeneous. -->
<!-- ::: {.example} -->
<!-- **Example 13**. *(Adapted from W.J.Stewart) -->
<!-- [\[exampleStationary2\]]{#exampleStationary2 label="exampleStationary2"} -->
<!-- A Markov chain $X_0, X_1, \ldots$ consists of two states $\{1,2\}$. At -->
<!-- time step $n$, the probability that the Markov chain remains in its -->
<!-- current state is given by $$p_{11}(n) = p_{22}(n) = 1/n,$$ while the -->
<!-- probability that it changes state is given by -->
<!-- $$p_{12}(n) = p_{21}(n) = 1 - 1/n.$$* -->
<!-- 1.  *Draw a transition diagram of the Markov chain.* -->
<!-- 2.  *Write down the transition matrix.* -->
<!-- 3.  *Calculate $\Pr(X_5 = 2, X_4 = 2, X_3 = 1, X_2 =1 | X_1 = 1)$.* -->
<!-- ::: -->
<!-- 1.  The transition diagram and the transition matrix are dependent of -->
<!--     the time step $n$, and are given as follows: -->
<!--     $$P(n) = \begin{bmatrix} -->
<!--         \frac{1}{n} & \frac{n-1}{n}   \\ -->
<!--         \frac{(n-1)}{n} & \frac{1}{n}   \\ -->
<!--         %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--         %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!--     \end{bmatrix}.$$ -->
<!-- 2.  The probability of taking a particular part can be calculated by -->
<!--     $$\begin{aligned} -->
<!--         \Pr(X_5 = 2, X_4 = 2, X_3 = 1, X_2 =1 | X_1 = 1)  &= p_{11}(1) \cdot p_{11}(2) \cdot p_{12}(3) \cdot p_{22}(4) \\ -->
<!--             &= 1 \cdot 1/2 \cdot 2/3 \cdot 1/4 = 1/12.\end{aligned}$$ -->
<!--     Other paths lead to state 2 after four transitions, and have -->
<!--     different probabilities according to the route they follow. What is -->
<!--     important is that, no matter which route is chosen, once the Markov -->
<!--     chain arrives in state 2 after four steps, the future evolution is -->
<!--     specified by $P(5)$, and not any other $P(i), i \le 4$. -->
<!-- Simulation -->
<!-- ========== -->
<!-- Simulation is a powerful tool for studying Markov chains. For many -->
<!-- Markov chains in real-world applications, state spaces are large and -->
<!-- matrix methods may not be practical. -->
<!-- A Markov chain can be simulated from an initial distribution and -->
<!-- transition matrix. To simulate a Markov sequence $X_0,X_1, \ldots,$ -->
<!-- simulate each random variable sequentially conditional on the outcome of -->
<!-- the previous variable. That is, first simulate $X_0$ according to the -->
<!-- initial distribution. If $X_0 = i$, then simulate $X_1$ from the $i-th$ -->
<!-- row of the transition matrix. If $X_1 = j$, then simulate $X_2$ from the -->
<!-- $j-th$ row of the transition matrix, and so on. -->
<!--     [frame=single, escapeinside={(*}{*)}, caption={Algorithm for Simulating a Markov Chain}] -->
<!--     Input: (i) initial distribution  (*$\boldsymbol{\mu}$*), (ii) transition matrix (*$P$*), (iii) number of steps (*$n$*). -->
<!--     Output: (*$X_0, X_1, \ldots , X_n$*) -->
<!--     Algorithm: -->
<!--         Generate (*$X_0$*) according to (*$\boldsymbol{\mu}$*) -->
<!--         FOR (*$i = 1, \ldots ,n$*) -->
<!--             Assume that (*$X_{i-1} = j$*) -->
<!--             Set (*$\boldsymbol p = j-$*)th row of (*$P$*) -->
<!--             Generate (*$X_i$*) according to (*$\boldsymbol p$*) -->
<!--         END FOR -->
<!-- Monte Carlo Methods -->
<!-- ------------------- -->
<!-- Monte Carlo methods are simulation-based algorithms that rely on -->
<!-- generating a large set of samples from a statistical model to obtain the -->
<!-- behaviour of the model and estimate the quantities of interest. For a -->
<!-- large sample set of a random variable representing a quantity of -->
<!-- interest, the law of large numbers allows to approximate the expectation -->
<!-- by the average value from the samples. -->
<!-- Consider repeated independent trials of a random experiment. We will -->
<!-- need to generate a large number of samples $X_1, X_2, \ldots$ from the -->
<!-- model. A Monte Carlo method for estimating the expectation -->
<!-- $\mathrm{E}( X )$ is a numerical method based on the approximation -->
<!-- $$\mathrm{E}(X) \approx \frac{1}{N}\sum_{i=1}^N X_i,$$ where -->
<!-- $X_1, X_2, \ldots$ are i.i.d. with the same distribution as $X$. -->
<!-- While computing expectations and computing probabilities at first look -->
<!-- like different problems, the latter can be reduced to the former: if $X$ -->
<!-- is a random variable, we have -->
<!-- $$\Pr(X \in A) = \mathrm{E}(\mathbbm{1}_A(X)).$$ -->
<!-- Using this equality, we can estimate $\Pr(X \in A)$ by -->
<!-- $$\Pr(X \in A) = \mathrm{E}(\mathbbm{1}_A(X)) = \frac{1}{N}\sum_{i=1}^N \mathbbm{1}_A(X_i).$$ -->
<!-- Recall that the indicator function of the set $A$ is the defined as -->
<!-- $$\mathbbm{1}_A(x) = -->
<!-- \begin{cases*} -->
<!--      1          & if $x \in A $ \\ -->
<!--       0        & otherwise. -->
<!-- \end{cases*}$$ -->
<!-- The following user-defined function in Excel can be used to simulate -->
<!-- random numbers from a discrete distribution. -->
<!--     [frame=single, escapeinside={(*}{*)}, caption={A user-defined function in Excel to simulate random numbers from a discrete distribution. -->
<!--     }] -->
<!--     Public Function Discrete(value As Variant, prob As Variant) -->
<!--     Dim i As Integer -->
<!--     Dim cumProb As Single -->
<!--     Dim uniform As Single -->
<!--     Randomize -->
<!--     'Randomize Statement -->
<!--     'Initializes the random-number generator. -->
<!--     Application.Volatile -->
<!--     ' This example marks the user-defined function Discrete as volatile. -->
<!--     ' The function will be recalculated when any cell in any workbook -->
<!--     ' in the application window changes value worksheet. -->
<!--     uniform = Rnd -->
<!--     cumProb = prob(1) -->
<!--     i = 1 -->
<!--     Do Until cumProb > uniform -->
<!--         i = i + 1 -->
<!--         cumProb = cumProb + prob(i) -->
<!--     Loop -->
<!--     Discrete = value(i) -->
<!--     End Function -->
<!-- ::: {.example} -->
<!-- **Example 14**. *(R or Excel) A gambler starts with and plays a game -->
<!-- where the chance of winning each round is 60%. The gambler either wins -->
<!-- or loses on each round. The game stops when the gambler either gains or -->
<!-- goes bust.* -->
<!-- 1.  *Develop an Excel worksheet or create an R code to simulate 50 steps -->
<!--     of the finite Markov chain of the random walk $X_n$ given in -->
<!--     Example [Example 12](#simpleRW){reference-type="ref" -->
<!--     reference="simpleRW"}. Repeat the simulation 10 times. How many of -->
<!--     your simulations end at 0.* -->
<!-- 2.  *Use the results from the simulations to estimate the mean and -->
<!--     variance of $X_{5}$.* -->
<!-- 3.  *Use the results from the simulations to estimate the probability -->
<!--     that the gambler is eventually ruined.* -->
<!-- ::: -->
<!-- ::: {#exampleStationary2 .example} -->
<!-- **Example 15**. *(R or Excel) A Markov chain $X_0, X_1, \ldots$ on -->
<!-- states $\{1,2\}$ has the following transition matrix -->
<!-- $$P = \begin{bmatrix} -->
<!--     1-a & a   \\ -->
<!--     b & 1-b   \\ -->
<!--     %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix},$$ where $0 < a,b < 1.$* -->
<!-- 1.  *Use either Excel or R to estimate the long-term distribution of the -->
<!--     Markov chain. (Hint: consider the $n$-step transition matrix for -->
<!--     several increasing values of $n$). Comments on the results -->
<!--     obtained.* -->
<!--     *Later we will see that in many cases, a Markov chain exhibits a -->
<!--     long-term limiting behaviour. The chain settles down to an -->
<!--     equilibrium distribution, which is independent of its initial -->
<!--     state.* -->
<!-- 2.  *Use simulations to estimate the long-term probability that a Markov -->
<!--     chain hits each of the states. (Hint: simulate the Markov chain 1000 -->
<!--     steps and calculate the proportion of visits to each state)* -->
<!-- ::: -->
<!-- Classification of states -->
<!-- ======================== -->
<!-- Throughout this section $\{ X_n\}_{n \ge 0}$ is a time homogeneous -->
<!-- Markov chain with state space $S$ and transition matrix -->
<!-- $P = (p_{ij})_{i,j \in S}$. -->
<!-- For any $i, j \in S$, -->
<!-- -   The state $j$ can be **reached** from the state $i$, denoted by -->
<!--     $i \rightarrow j$ if there is a nonzero probability -->
<!--     $p^{(n)}_{ij} > 0$ for some $n \ge 0$. -->
<!-- -   The states $i$ and $j$ are said to **communicate**, or to be **in -->
<!--     the same class**, and denoted by $i \leftrightarrow j$, if -->
<!--     $i \rightarrow j$ and $j \rightarrow i$. -->
<!-- Note that $i \rightarrow j$ if and only if there exist states -->
<!-- $k_1, k_2, \ldots, k_r$ such that -->
<!-- $$p_{ik_1} p_{k_1k_2} \ldots p_{k_r j}>0,$$ i.e. it is not necessary -->
<!-- that $j$ can be reached from the state $i$ in one single step. -->
<!-- -   The relation $\leftrightarrow$ is an equivalence relation and -->
<!--     partition the state space $S$ into equivalence classes, which are -->
<!--     known as **classes (or communication classes)** of the Markov chain. -->
<!--     Thus in any class all the states communicate, but none of them -->
<!--     communicates with any state outside the class. -->
<!-- Additional properties for a communication class are defined as follows: -->
<!-- -   The class $C$ is said to be **closed** if it is impossible to reach -->
<!--     any state outside $C$ from any state in $C$, i.e. escape from $C$ is -->
<!--     impossible. Otherwise, the class $C$ is said to be **non-closed**, -->
<!--     i.e. escape from $C$ is possible. -->
<!-- ```{=html} -->
<!-- <!-- -->
<p>–&gt;
<!-- ``` -->
<!-- -   If the entire state space $S$ is only one communication class (all -->
<!--     states communicate), then it is necessarily closed and the Markov -->
<!--     chain is said to be **irreducible**. Otherwise, the Markov chain is -->
<!--     said to be **reducible**. --></p>
<!-- -   A closed class consisting of a single state is an **absorbing -->
<!--     state**. -->
<!-- ::: {.example} -->
<!-- **Example 16**. *Consider each of the following Markov chains:* -->
<!-- (a) *NCD system (Example [Example 2](#NCD){reference-type="ref" -->
<!--     reference="NCD"}),* -->
<!-- (b) *the health insurance system (Example -->
<!--     [Example 3](#healthInsurance){reference-type="ref" -->
<!--     reference="healthInsurance"}), and* -->
<!-- (c) *a simple random walk (Example -->
<!--     [Example 12](#simpleRW){reference-type="ref" -->
<!--     reference="simpleRW"}),* -->
<!-- *Identify the communication classes. Is the Markov chain irreducible?* -->
<!-- ::: -->
<!-- It is a good practice to use transition diagram and also verify the -->
<!-- answers. -->
<!-- (a) Every two states are intercommunicating, so $\{0,1,2\}$ is a single -->
<!--     closed class, and hence the Markov chain is irreducible. This is -->
<!--     because $0 \rightarrow 1$ and $1 \rightarrow 0$ ($p_{01} > 0$ and -->
<!--     $p_{10} > 0$), and $1 \rightarrow 2$ and $2 \rightarrow 1$ -->
<!--     ($p_{12} > 0$ and $p_{21} > 0$). -->
<!-- (b) There are two classes of intercommunicating states, $O = \{H,S\}$ is -->
<!--     non-closed, and $C = \{ D \}$ is closed (and also an absorbing -->
<!--     state). Clearly, the Markov chain is not irreducible. This is -->
<!--     because -->
<!--     -   $p_{DD} = 1$, i.e. $C$ is a class. -->
<!--     -   $O$ is open class because $p_{HS} > 0$, $p_{SH} >0$, so this is -->
<!--         a class, and for example $p_{HD}$ \> 0 but $p^{(n)}_{DH} = 0$ -->
<!--         for all $n$ (i.e. one cannot leave $C$ starting from the state -->
<!--         $D$), so O is an open class. -->
<!-- (c) The simple random walk with absorbing boundaries has three classes, -->
<!--     $\{1,2, \ldots, N-1 \}$ is non-closed class, $\{0 \}$ and $\{N \}$ -->
<!--     are two closed classes. -->
<!-- ::: {#exampleMC .example} -->
<!-- **Example 17**. *A Markov chain with state space $S = \{1,2,3,4,5\}$ has -->
<!-- the following transition matrix: $$P = \begin{bmatrix} -->
<!--     1 & 0 & 0 & 0 & 0     \\ -->
<!--     1/5 & 1/5 & 1/5 & 1/5 & 1/5   \\ -->
<!--     1/3 & 1/3 & 0 & 1/3 & 0     \\ -->
<!-- 0 & 0 & 0 & 0 & 1     \\ -->
<!-- 0 & 0 & 0 & 1/2 & 1/2     \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix}.$$* -->
<!-- 1.  *Draw a transition diagram.* -->
<!-- 2.  *Identify the communication classes. Is the Markov chain -->
<!--     irreducible?* -->
<!-- ::: -->
<!-- There are two closed classes $C_1 = \{1\}$ and $C_2 = \{4,5\}$ and one -->
<!-- non-closed class $O = \{ 2,3\}$. This is because -->
<!-- -   $C_1$ is closed because $p_{11} = 1$. -->
<!-- -   $C_2$ is a class because $4 \rightarrow 5$ ($p_{45} >0$) and -->
<!--     $5 \rightarrow 4$ ($p_{54} >0$), and is closed because $p_{ij} = 0$ -->
<!--     for all $i \in C_2$ and $j \not\in C_2$. -->
<!-- -   $O$ is a class because $2 \rightarrow 3$ ($p_{23} >0$) and -->
<!--     $3 \rightarrow 2$ ($p_{32} > 0$), and is non-closed because -->
<!--     $p_{21} > 0$, but $p_{11} = 1$. -->
<!-- Absorption probabilities and expected time to absorption -->
<!-- ======================================================== -->
<!-- For the random walk with absorbing boundaries (i.e. $0$ and $N$), two -->
<!-- questions arises, in which state, $0$ or $N$ is the process eventually -->
<!-- absorbed (or trapped) and on the average how long does it take to reach -->
<!-- one of these absorbing states? We first define the following terms which -->
<!-- applies to the random walk process with absorbing boundaries. -->
<!-- The time of absorption $T$ is defined as -->
<!-- $$T = \min\{ n \ge 0 | X_n = 0 \text{ or } X_n = N \}$$ and the -->
<!-- **probability of eventually absorption** in state 0 is given by -->
<!-- $$u_i = \Pr\{ X_T = 0 | X_0 = i  \}, \text{ for } i = 1,2,\ldots,N-1.$$ -->
<!-- The **mean time to absorption** of the process is given by -->
<!-- $$\mathrm{E}[T |  X_0 = i ] \, \text{ for } i = 1,2,\ldots,N-1.$$ -->
<!-- First step analysis -->
<!-- ------------------- -->
<!-- First step analysis allows us to evaluate quantities of interest from -->
<!-- the Markov chain, for e.g. the absorption probabilities and the mean -->
<!-- duration until absorption. The method is based on considering all -->
<!-- possibilities at the end of the first transition and then apply the law -->
<!-- of total probability to formulate equations involved all unknown -->
<!-- quantities. We illustrate how to use the first step analysis in the -->
<!-- following Markov chain. -->
<!-- ::: {#eg_absorption .example} -->
<!-- **Example 18**. *Consider the Markov chain with state space -->
<!-- $S = \{0,1,2\}$ and transition probability matrix given by -->
<!-- $$P = \begin{bmatrix} -->
<!--     1 & 0 & 0    \\ -->
<!--     p_{10} & p_{11} & p_{12}   \\ -->
<!--    0 & 0 & 1   \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix}.$$* -->
<!-- ::: -->
<!-- The classes and types are as follows: -->
<!-- -   Two closed classes are $C_1 = \{0\}$ and $C_2 = \{2\}$. -->
<!-- -   $\{1\}$ is a non-closed class. -->
<!-- Let us consider the problem of evaluating the absorption probabilities. -->
<!-- For any closed class $C$, define -->
<!-- $$u^C_{i} = \Pr(\text{Markov chain eventually absorbed in } C | X_0 = i).$$ -->
<!-- Clearly, the absorption probabilities also depend on the initial states. -->
<!-- A vector of absorption probabilities is then given by -->
<!-- $\mathbf{u}^C = (u^C_i)_{i \in S}$ We suppress the superscript $C$ and -->
<!-- simply write $u^C_{i} = u_i$ and $\mathbf{u}^C = \mathbf{u}$. -->
<!-- Consider the closed class $C_1 = \{0\}$. We have $$\begin{aligned} -->
<!--     u_{0} &= \Pr(\text{Markov chain eventually absorbed in } C_1 | X_0 = 0) = 1, \\ -->
<!--     u_{2} &= \Pr(\text{Markov chain eventually absorbed in } C_1 | X_0 = 2) = 0, \\ -->
<!--     u_{1} &= \Pr(\text{Markov chain eventually absorbed in } C_1 | X_0 = 1) = u_1. \\   \end{aligned}$$ -->
<!-- By considering the first transition from state $1$ to either state 0, 1 -->
<!-- and 2, and using the Markov property, the law of total probability gives -->
<!-- $$\begin{aligned} -->
<!--     u_1 &= \Pr(\text{Markov chain eventually absorbed in } C_1 | X_0 = 1) \\ -->
<!--     &= \sum_{k = 0}^2 \Pr(\text{Markov chain eventually absorbed in } C_1 | X_0 = 1, X_1 = k)   \Pr(X_1 = k | X_0 = 1) \\ -->
<!--     &= \sum_{k = 0}^2 \Pr(\text{Markov chain eventually absorbed in } C_1 | X_1 = k)    \Pr(X_1 = k | X_0 = 1) \\ -->
<!--     &=  (p_{10}) \cdot  u_0 + (p_{11}) \cdot  u_1 + (p_{12}) \cdot  u_2  \\ -->
<!--     &= (p_{10})\cdot 1 + (p_{11}) \cdot  u_1 +  (p_{12}) \cdot 0. \end{aligned}$$ -->
<!-- Solving for $u_1$ gives -->
<!-- $$u_1 =  u^{C_1}_1 = \frac{p_{10}}{1 - p_{11}} = \frac{p_{10}}{p_{10} +  p_{12}}.$$ -->
<!-- Similarly, we have $$\begin{aligned} -->
<!--     u_0 &= p_{00} \cdot u_0 + p_{01} \cdot  u_1 +  p_{02} \cdot  u_2 \\ -->
<!--     u_1 &= p_{10} \cdot  u_0 + p_{11} \cdot  u_1 + p_{12} \cdot  u_2  \\ -->
<!--     u_2 &= p_{20} \cdot  u_0 + p_{21} \cdot  u_1 + p_{22} \cdot  u_2,  \\\end{aligned}$$ -->
<!-- where the first and the last equations reduce to $u_0 = u_0$ and -->
<!-- $u_2 = u_2$, respectively. In general, for a closed class $C$, the -->
<!-- vector of absorption probabilities $\mathbf{u}$ satisfies the following -->
<!-- system of linear equations: -->
<!-- 1.  $\mathbf{u} = P\mathbf{u}$ (here $\mathbf{u}$ is treated as a column -->
<!--     vector), -->
<!-- 2.  $u_{i} = \Pr(\text{Markov chain eventually absorbed in } C) | X_0 = i) = 1$ -->
<!--     for all $i \in C$, and -->
<!-- 3.  $u_{i} = \Pr(\text{Markov chain eventually absorbed in } C) | X_0 = i) = 0$ -->
<!--     for all $i$ in any other close classes. -->
<!-- ::: {#absorption .example} -->
<!-- **Example 19**. *In this example, consider the closed class -->
<!-- $C_2 = \{2\}$. Find the absorption probabilities $u^{C_2}_0, u^{C_2}_1$ -->
<!-- and $u^{C_2}_2$. Comment on these results.* -->
<!-- ::: -->
<!-- For the closed class $C_2$, we proceed in the same way as in the closed -->
<!-- class $C_1$. Let $\mathbf{u}=\mathbf{u}^{C_{2}} = (u_0,u_1,u_2)^T$ be -->
<!-- the vector of absorption probabilities in the closed class $C_{2}=\{2\}$ -->
<!-- with $u_0 = 0$ and $u_2 =1$. It follows that $$\begin{aligned} -->
<!--     u_1 &=  p_{10} \cdot  u_0 + p_{11} \cdot  u_1 + p_{12} \cdot  u_2  \\ -->
<!--     &=  p_{11} \cdot  u_1 + p_{12}.\end{aligned}$$ Hence, -->
<!-- $u_1 = \frac{p_{12}}{1-  p_{11}} =\frac{p_{12}}{p_{10}+ p_{12}}$. It -->
<!-- should be emphasised that -->
<!-- $$\mathbf{u}^{C_1} + \mathbf{u}^{C_2} = \mathbf{1} .$$ -->
<!-- 1.  For any initial state $i$, the sum of the absorption probabilities -->
<!--     over all closed classes is 1 (as verified in Example -->
<!--     [Example 19](#absorption){reference-type="ref" -->
<!--     reference="absorption"}). In particular, when a Markov chain has two -->
<!--     closed classes $C_1$ and $C_2$, -->
<!--     $\mathbf{u}^{C_2} = \mathbf{1} - \mathbf{u}^{C_1}$. -->
<!-- 2.  In the case when $S$ is finite or when the set of states in -->
<!--     non-closed classes is finite, the vector $\mathbf{u}$ is the unique -->
<!--     solution of the above system of linear equations. -->
<!-- ::: {.example} -->
<!-- **Example 20**. *Consider the Markov chain defined in Example -->
<!-- [Example 17](#exampleMC){reference-type="ref" reference="exampleMC"}. -->
<!-- Find the absorption probabilities in the closed class $C_1 = \{1\}$ and -->
<!-- $C_2 = \{4,5\}$.* -->
<!-- ::: -->
<!-- Here $C_{1}=\{1\}$ and $C_{2}=\{4,5\}$ are closed classes and -->
<!-- $0 = \{2,3\}$ is an open class. -->
<!-- Let $\mathbf{u}=\mathbf{u}^{C_{1}}$ be the vector of absorption -->
<!-- probabilities in the closed class $C_{1}=\{1\}$. Write -->
<!-- $\mathbf{u}= (u_1,u_2, \ldots,u_5)^T$ and $u_1 = 1$ and $u_4 =  u_5 =0$, -->
<!-- From $\mathbf{u}=P \cdot \mathbf{u}$, -->
<!-- $$\left(\begin{array}{c}u_{1} \\ u_{2} \\ \vdots \\ u_{5}\end{array}\right)=P\left(\begin{array}{c}u_{1} \\ u_{2} \\ \vdots \\ u_{5}\end{array}\right) \text {gives}$$ -->
<!-- $$\begin{aligned} -->
<!--     u_2 &= \frac{1}{5} + \frac{1}{5} u_2 + \frac{1}{5} u_3 \\ -->
<!--     u_3 &= \frac{1}{3} + \frac{1}{3} u_2.   \end{aligned}$$ Solving the -->
<!-- linear system for $u_2$ and $u_3$ yields $u_2 = 4/11$ and $u_3 = 5/11$. -->
<!-- Hence, the absorption probabilities in the closed class $C_1$ is -->
<!-- $$\mathbf{u}= (1,4/11,5/11,0,0)^T.$$ In addition, since there are two -->
<!-- closed classes, -->
<!-- $\mathbf{u}^{C_2} = \mathbf{1} - \mathbf{u}^{C_1} = (0,7/11,6/11,1,1)^T.$ -->
<!-- The expected time to absorption -->
<!-- ------------------------------- -->
<!-- The expected time to absorption can be determined by analysing all -->
<!-- possibilities occurring in the first step. We again consider the process -->
<!-- defined in Example [Example 18](#eg_absorption){reference-type="ref" -->
<!-- reference="eg_absorption"} on the set $\{0, 1, 2\}$ with the transition -->
<!-- matrix $$P = \begin{bmatrix} -->
<!--     1 & 0 & 0    \\ -->
<!--     p_{10} & p_{11} & p_{12}   \\ -->
<!--    0 & 0 & 1   \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix}.$$ The time of absorption $T$ is defined as -->
<!-- $$T = \min\{ n \ge 0 | X_n = 0 \text{ or } X_n = 2 \}$$ and the mean -->
<!-- time to absorption of the process is given by -->
<!-- $v = \mathrm{E}[T |  X_0 = 1 ] .$ -->
<!-- The following observations can be made: -->
<!-- 1.  The absorption time $T$ is always at least 1. -->
<!-- 2.  If either $X_1 = 0$ or $X_1 = 2$, then no further steps are -->
<!--     required. -->
<!-- 3.  If $X_1 = 1$, then the process is back at its starting point and on -->
<!--     the average $v$ additional steps are required for absorption. -->
<!-- Weighting all these possibilities by their respective probabilities, we -->
<!-- obtain the following equation $$\begin{aligned} -->
<!-- v &= 1 + p_{10} \cdot 0 + p_{11} \cdot v  + p_{12} \cdot 0    \\ -->
<!--   &= 1 +  p_{11} \cdot v,\end{aligned}$$ which results in -->
<!-- $$v = \frac{1}{1 - p_{11}}.$$ -->
<!-- ::: {#eg_absorption2 .example} -->
<!-- **Example 21**. *Consider the Markov chain with state space -->
<!-- $S = \{0,1,2,3\}$ and transition probability matrix given by -->
<!-- $$P = \begin{bmatrix} -->
<!--     1 & 0 & 0  & 0  \\ -->
<!--     p_{10} & p_{11} & p_{12} & p_{13}   \\ -->
<!--     p_{20} & p_{21} & p_{22} & p_{23}   \\ -->
<!--    0 & 0 & 0 & 1   \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix}.$$ Let $T$ be the time of absorption defined by -->
<!-- $$T = \min\{ n \ge 0 | X_n = 0 \text{ or } X_n = 3 \}$$ and the -->
<!-- **absorption probabilities** given by -->
<!-- $$u_i = \Pr\{ X_T = 0 | X_0 = i  \}, \text{ for } i = 1,2$$ and the mean -->
<!-- time to absorption of the process is given by -->
<!-- $$v_i  = \mathrm{E}[T |  X_0 = 1 ] , \text{ for } i = 1,2.$$ Calculate -->
<!-- the absorption probabilities and the mean time to absorption.* -->
<!-- ::: -->
<!-- There are 2 closed classes including $C_1= \{ 0 \}$, and $C_2= \{ 3 \}$, -->
<!-- and one non-closed class $O = \{1,2 \}$. Here, -->
<!-- $$u_i = u_i^{C_1} = \Pr\{ X_T = 0 | X_0 = i  \} = \Pr(\text{Markov chain eventually absorbed in } C_1 | X_0 = i), \text{ for } i = 1,2$$ -->
<!-- By conditioning on the first step from state $i$ and using the Markov -->
<!-- property, we have $$u_i^{C_1} = \sum_{j \in S} p_{ij} u_j^{C_1}.$$ -->
<!-- Clearly, $u_0^{C_1} = 1$ and $u_3^{C_1} = 0$. In particular, we have -->
<!-- $$\begin{aligned} -->
<!--     u_1 &= p_{10} \cdot 1 + p_{11} \cdot u_1 + p_{12} \cdot u_2 \\ -->
<!--     u_2 &= p_{20} \cdot 1 + p_{21} \cdot u_1 + p_{22} \cdot u_2, \\\end{aligned}$$ -->
<!-- which can also be obtained from the matrix equation -->
<!-- $\mathbf{u} = P\mathbf{u}$, where $\mathbf{u} = (1, u_1, u_2, 0)^T$. The -->
<!-- solution to the system of linear equations is $$\begin{aligned} -->
<!--     u_1 &= \frac{ p_{10} (p_{22} - 1) - p_{12} p_{20}  }{ p_{11}(-p_{22}) +  p_{11}  + p_{12} p_{21}  + p_{22} - 1}, \\ -->
<!--     u_2 &= \frac{(  p_{11} - 1) p_{20} - p_{10} p_{21}  }{    p_{11}(-p_{22})  + p_{11}  + p_{12} p_{21} + p_{22} -1  }  . \\\end{aligned}$$ -->
<!-- Similarly, the mean time to absorption also depends on the starting -->
<!-- state. By the first step analysis, we have for -->
<!-- $v_i  = \mathrm{E}[T |  X_0 = 1 ]$, $$\begin{aligned} -->
<!--     v_1 &=  1 + p_{11} \cdot v_1 + p_{12} \cdot v_2 \\ -->
<!--     v_2 &=  1 + p_{21} \cdot v_1 + p_{22} \cdot v_2. \\\end{aligned}$$ -->
<!-- Here the absorption time $T$ is always at least 1. If either $X_1 = 0$ -->
<!-- or $X_1 = 3$, then no further steps are required. On the other hand, if -->
<!-- $X_1 = 1$ or $X_1 = 2$, then the process will require additional steps, -->
<!-- and on the average, these are $v_1$ and $v_2$. Weighting these two -->
<!-- possibilities, i.e. whether $X_1 = 1$ or $X_1 = 2$, by their respective -->
<!-- probabilities and summing according to the law of total probability -->
<!-- result in the above system of equations. -->
<!-- Solving the equations for $v_1$ and $v_2$ give the mean time to -->
<!-- absorption $$\begin{aligned} -->
<!--     v_1 &=  \frac{-p_{12}  + p_{22} -1}{p_{11}(-p_{22}) + p_{11} + p_{12}p_{21} + p_{22} - 1}, \\ -->
<!--     v_2 &=  \frac{p_{11}  - p_{21} -1}{p_{11}(-p_{22}) + p_{11} + p_{12}p_{21} + p_{22} - 1}. \\\end{aligned}$$ -->
<!-- The long-term distribution of a Markov chain -->
<!-- ============================================ -->
<!-- In this section, we present another important property concerning -->
<!-- limiting behaviour of $P^n$ as $n \rightarrow \infty$ and hence the -->
<!-- long-term distribution of a Markov chain satisfying some certain -->
<!-- conditions. In particular, some Markov chains will converge to an -->
<!-- equilibrium (limiting) distribution, which is independent of its initial -->
<!-- state. -->
<!-- We also assume that the Markov chain with **a single closed class** $S$. -->
<!-- Stationary and limiting distributions for a single closed class -->
<!-- --------------------------------------------------------------- -->
<!-- ### Stationary distributions {#stationary-distributions .unnumbered} -->
<!-- Throughout this section, we consider a Markov chain whose transition -->
<!-- probability matrix is $P$ and state space $S$ is a single close class. -->
<!-- Then $S$ is necessarily closed and hence irreducible. -->
<!-- A probability distribution $\boldsymbol{\pi} = (\pi)_{i \in S}$ on $S$ -->
<!-- is **stationary** if the following conditions hold (here -->
<!-- $\boldsymbol{\pi}$ is a row vector): -->
<!-- 1.  $\pi_j = \sum_{i \in S} \pi_i p_{ij}$ or equivalently -->
<!--     $\boldsymbol{\pi}  P  =\boldsymbol{\pi}$, -->
<!-- 2.  $\pi_j \ge 0$, -->
<!-- 3.  $\sum_{j \in S} \pi_j = 1$. -->
<!-- ```{=html} -->
<!-- <!-- -->
<p>–&gt;
<!-- ``` -->
<!-- 1.  For any stationary distribution $\boldsymbol{\pi}$, for all -->
<!--     $n \ge 1$, $$\boldsymbol{\pi}  P^n  =\boldsymbol{\pi}.$$ Therefore, -->
<!--     if we take $\boldsymbol{\pi}$ as the initial probability -->
<!--     distribution, i.e. $\Pr(X_0 = i) = \pi_i$, then then the -->
<!--     distribution of $X_n$ is also $\boldsymbol{\pi}$ , i.e. -->
<!--     $\Pr(X_n = i) = \pi_i$ --></p>
<!-- 2.  The probability distribution $\boldsymbol{\pi}$ is said to be an -->
<!--     invariant probability distribution. -->
<!-- 3.  The most important property concerning the stationary -->
<!--     distribution(which will be made formal) is that it gives the -->
<!--     **long-term (limiting) distribution** of a Markov chain. In -->
<!--     addition, $\pi_j$ also gives **the long run mean fraction of time** -->
<!--     that the process $\{X_n\}$ is in state $j$. -->
<!-- ::: {#exampleStationary .example} -->
<!-- **Example 22**. *A Markov chain $X_0, X_1, \ldots$ on states $\{1,2\}$ -->
<!-- has the following transition matrix $$P = \begin{bmatrix} -->
<!--     1-a & a   \\ -->
<!--     b & 1-b   \\ -->
<!--     %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix},$$ where $0 < a,b < 1.$* -->
<!-- 1.  *Show that $$P^n = \frac{1}{a+b} \begin{bmatrix} -->
<!--         b & a   \\ -->
<!--         b & a   \\ -->
<!--         %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--         %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!--     \end{bmatrix} + -->
<!--     \frac{(1-a-b)^n}{a+b} \begin{bmatrix} -->
<!--         a & -a   \\ -->
<!--         -b & b   \\ -->
<!--         %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--         %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!--     \end{bmatrix}.$$* -->
<!-- 2.  *Show that the stationary probability distribution is -->
<!--     $$\boldsymbol{\pi} = \left( \frac{b}{a+b},  \frac{a}{a+b} \right)  .$$* -->
<!-- 3.  *Show that -->
<!--     $$\lim_{n \rightarrow \infty} p_{ij}^{(n)} = \pi_j > 0 , \text{ for }j \in \{1,2\}.$$* -->
<!-- ::: -->
<!-- ### Proportion of Time in Each State {#proportion-of-time-in-each-state .unnumbered} -->
<!-- The limiting distribution provides the long-term behaviour of the Markov -->
<!-- chain, i.e. it is the long-term probability that a Markov chain hits -->
<!-- each state. In this section, it can be shown that it also gives the -->
<!-- long-term proportion of time that the chain visits each state. Let us -->
<!-- consider a Markov chain $X_0, X_1, \ldots$ whose transition probability -->
<!-- matrix is $P$ and its limiting distribution is $\boldsymbol{\pi}$. Note -->
<!-- that the limiting distribution for the Markov chain satisfies -->
<!-- $$\lim_{n \rightarrow \infty}  p^{(n)}_{ij} =  \pi_j.$$ -->
<!-- For each state $j$, define indicator random variable $$I_k = -->
<!--     \begin{cases*} -->
<!--       1, & if $X_k = j$ \\ -->
<!--       0,  & otherwise, -->
<!--     \end{cases*}$$ for $k = 0,1,\ldots$. Hence, the number of times that -->
<!-- the Markov chain visits $j$ in the first $n$ steps is given by -->
<!-- $\sum_{k =0}^{n-1} I_k$ and the expected long-term proportion of time -->
<!-- that the chain visits state $j$ given that its initial state is $i$ is -->
<!-- $$\begin{aligned} -->
<!--     \lim_{n \rightarrow \infty} \mathrm{E}\left(\frac{1}{n}  \sum_{k =0}^{n-1} I_k \, |\, X_0 = i \right) &= -->
<!--     \lim_{n \rightarrow \infty}  \frac{1}{n}  \sum_{k =0}^{n-1} \mathrm{E}( I_k \, |\, X_0 = i ) \\ -->
<!--     &= \lim_{n \rightarrow \infty}  \frac{1}{n}  \sum_{k =0}^{n-1} \Pr(X_k = j | X_0 = i)\\ -->
<!--     &= \lim_{n \rightarrow \infty}  \frac{1}{n}  \sum_{k =0}^{n-1} p^{(k)}_{ij}\\ -->
<!--     &= \lim_{n \rightarrow \infty}  p^{(n)}_{ij} =  \pi_j.\end{aligned}$$ -->
<!-- Here we use the fact that if the sequence of numbers converges to a -->
<!-- limit, i.e. $x_n \rightarrow x$ as $n \rightarrow \infty$, then the -->
<!-- sequence of partial averages also converges to that limit, i.e. -->
<!-- $(x_1 + x_2 + \cdots x_n)/n \rightarrow x$ as $n \rightarrow \infty$. -->
<!-- This result is known as Cesaro's lemma. -->
<!-- ::: {#weatherExample2 .example} -->
<!-- **Example 23**. *Recall from -->
<!-- Example [Example 9](#weather){reference-type="ref" reference="weather"}, -->
<!-- the simple weather pattern can be classified into three types including -->
<!-- rainy ($R$), cloudy ($C$) and sunny ($S$). The weather is observed daily -->
<!-- and can be modelled by the Markov transition matrix -->
<!-- $$P = \begin{bmatrix} -->
<!--     0.7 & 0.2 & 0.1    \\ -->
<!--     0.75 & 0.15 & 0.1   \\ -->
<!--    0.2 & 0.4 & 0.4   \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix}.$$ It can be shown that the stationary distribution and -->
<!-- also the limiting distribution of the Markov chain is -->
<!-- $$\boldsymbol{\pi} = (94/147, 32/147,    1/7)$$ which gives the -->
<!-- proportions of visits to rainy, cloudy and sunny states are 94/147, -->
<!-- 32/147, 1/7, respectively.* -->
<!-- ::: -->
<!-- ### The method of finding the stationary distribution {#the-method-of-finding-the-stationary-distribution .unnumbered} -->
<!-- To find the stationary distribution, we simply solve the linear -->
<!-- equations $\boldsymbol{\pi}  P  =\boldsymbol{\pi}$ (note that one of the -->
<!-- equations can be discarded), together with the condition -->
<!-- $\sum_{j \in S} \pi_j = 1$. -->
<!-- ::: {.example} -->
<!-- **Example 24**. *For the NCD process in Example -->
<!-- [Example 2](#NCD){reference-type="ref" reference="NCD"}, the Markov -->
<!-- chain has the following transition probability matrix -->
<!-- $$P = \begin{bmatrix} -->
<!--     1- p & p & 0    \\ -->
<!--     1-p & 0 & p   \\ -->
<!--     0 & 1-p & p    \\ -->
<!-- \end{bmatrix}.$$ Find the stationary probability distribution of this -->
<!-- chain.* -->
<!-- ::: -->
<!-- Denote the stationary probability distribution by -->
<!-- $\boldsymbol{\pi} =  (\pi_1,  \pi_2, \pi_3)$. From -->
<!-- $\boldsymbol{\pi}  P  =\boldsymbol{\pi}$ and -->
<!-- $\pi_1 +   \pi_2 +  \pi_3 = 1$, -->
<!-- $$(\pi_1,  \pi_2, \pi_3)  \begin{bmatrix} -->
<!--     1- p & p & 0    \\ -->
<!--     1-p & 0 & p   \\ -->
<!--     0 & 1-p & p    \\ -->
<!-- \end{bmatrix} =  (\pi_1,  \pi_2, \pi_3),$$ which is equivalent to -->
<!-- $$\begin{aligned} -->
<!-- (1- p)\pi_1 + (1- p)\pi_2 &= \pi_1 \\ -->
<!-- p\pi_1 + (1- p)\pi_3 &= \pi_2 \\ -->
<!-- p\pi_2 + p\pi_3 &= \pi_3 \end{aligned}$$ -->
<!-- By discarding one of the equations and adding the condition that -->
<!-- $\pi_1 +   \pi_2 +  \pi_3 = 1$, one can solve for -->
<!-- $\pi_1,  \pi_2, \pi_3$: -->
<!-- $$\pi_1 = \frac{(p-1)^2}{p^2 - p + 1},  \quad \pi_2 = \frac{-p^2 + p}{p^2 - p + 1},  \quad \pi_3 = \frac{p^2}{p^2 - p + 1}.$$ -->
<!-- 1.  In the above two examples, it can be shown that -->
<!--     $$\lim_{n \rightarrow \infty} p_{ij}^{(n)} = \pi_j > 0 , \text{ for }j \in S,$$ -->
<!--     or, in terms of the Markov chain $\{X_n\}$, -->
<!--     $$\lim_{n \rightarrow \infty} \Pr(X_n = j | X_0 = i) = \pi_j > 0, \text{ for }j \in S.$$ -->
<!--     This means that in the long run (as $n\rightarrow \infty$), the -->
<!--     probability of finding Markov chain in state $j$ is approximately -->
<!--     $\pi_j$ **no matter in which state the chain began at time 0**. This -->
<!--     property holds for some Markov chains which satisfy **\"certain -->
<!--     conditions\"**. -->
<!-- Sufficient conditions for the long-run behaviour of a Markov chain -->
<!-- ------------------------------------------------------------------ -->
<!-- In what follows, we will establish a set of sufficient conditions for -->
<!-- the long-run behaviour of a Markov chain. Two important results are -->
<!-- stated without proof. -->
<!-- ::: {.theorem} -->
<!-- **Theorem 1**. *A Markov chain with a finite state space has at least -->
<!-- one stationary probability distribution.* -->
<!-- ::: -->
<!-- ::: {.theorem} -->
<!-- **Theorem 2**. *An irreducible Markov chain with a finite state space -->
<!-- has a unique stationary probability distribution.* -->
<!-- ::: -->
<!-- ::: {#rwInfiniteS .example} -->
<!-- **Example 25**. *The simple random walks $X_n$ on -->
<!-- $S = \{\ldots, -2, -1, 0,1,2, \ldots \}$ is defined as -->
<!-- $$X_n = X_0 + \xi_1 + \xi_2 + \ldots + \xi_n,$$ where the random -->
<!-- variables $\xi_j$ are independent identically distributed with -->
<!-- $$\Pr(\xi = 1) = p, \quad \Pr(\xi = -1) = 1- p.$$ We can check that this -->
<!-- Markov chain is irreducible. However, the state space $S$ is infinite. -->
<!-- It can be checked directly from the equations -->
<!-- $\boldsymbol{\pi}  P  =\boldsymbol{\pi}$ that there is **no** stationary -->
<!-- distribution (given as an exercise).* -->
<!-- ::: -->
<!-- First we know that the entries of a stationary distribution sum to one. -->
<!-- Suppose the contrary that there is a stationary distribution -->
<!-- $\boldsymbol{\pi}$ for the simple random walk. Then by spatial -->
<!-- invariance of the simple random walk, $\pi_i$ is constant for all -->
<!-- $i \in S$ and also $\sum_{i\in S}\pi_i = 1$, which is impossible because -->
<!-- $S$ is infinite. Hence, there is **no** stationary distribution for the -->
<!-- simple random walk. -->
<!-- Limiting distributions {#limiting-distributions .unnumbered} -->
<!-- ---------------------- -->
<!-- One of the important properties of stationary distributions is that the -->
<!-- distribution of the Markov chain satisfying certain conditions converges -->
<!-- to the stationary distribution. This result provides the long-term -->
<!-- behaviour of the Markov chain. In order to state the main result of this -->
<!-- section, we need to introduce another concept, namely the period of a -->
<!-- state. -->
<!-- A state $i$ is said to be **periodic** with period $d > 1$ if a return -->
<!-- to $i$ is possible only in a number of steps that is a multiple of $d$. -->
<!-- Equivalently, the period $d$ is the greatest common divisor of all -->
<!-- integers $n$ for which $p^{(n)}_{ii} > 0$. If the greatest common -->
<!-- divisor is 1, the state has period 1 and is said to be **aperiodic**. -->
<!-- A Markov chain in which each state has period 1 is called **aperiodic**. -->
<!-- Most Markov chains in applications are aperiodic. -->
<!-- ::: {.example} -->
<!-- **Example 26**. *Is the NCD system in -->
<!-- Example [Example 2](#NCD){reference-type="ref" reference="NCD"} -->
<!-- aperiodic?* -->
<!-- ::: -->
<!-- The entire state space of the NCD system is a single class, and is -->
<!-- necessarily closed. Note also that $p_{00} > 0$ (and also $p_{22} > 0$), -->
<!-- i.e. the state 0 (and state 2) has an arrow back to itself. -->
<!-- Consequently, it is aperiodic because a return to this state is possible -->
<!-- in any number of steps (or the system can remain in this state in any -->
<!-- length of time). -->
<!-- Similarly, a return to state 1 is possible in $2,3, \ldots$ steps. -->
<!-- Therefore, the NCD system is aperiodic. -->
<!-- 1.  Periodicity is a class property, i.e. all states in one class have -->
<!--     the same period (or if $i \leftrightarrow j$, then $i$ and $j$ have -->
<!--     the same period). -->
<!-- 2.  The Markov chain $\{X_n\}_{n\ge 0}$ is aperiodic if and only if -->
<!--     there exists some $n >0$ such that -->
<!--     $$p^{(n)}_{ij} > 0 \text{ for all } i,j \in S.$$ Such Markov chain -->
<!--     is also called **regular**. -->
<!-- ::: {.example} -->
<!-- **Example 27**. *In the random walk model on a finite state space -->
<!-- $S = \{0,1,\ldots, N\}$ with absorbing boundaries in -->
<!-- Example [Example 12](#simpleRW){reference-type="ref" -->
<!-- reference="simpleRW"}, determine the period of each state.* -->
<!-- ::: -->
<!-- The transition diagram and the transition matrix of the simple random -->
<!-- walk with absorbing boundaries (states) are given as follows: -->
<!-- The simple random walk with absorbing boundaries has three classes, -->
<!-- $O = \{1, 2, \ldots, N - 1\}$ is non-closed class, $C_1 = {0}$ and -->
<!-- $C_2 = {N}$ are two closed classes. For each state $i$ in $O$, -->
<!-- $p^{(2n)}_{ii} > 0$ and $p^{(2n+1)}_{ii} = 0$ for $n = 1,2,\ldots$. -->
<!-- Therefore, the state $i$ in this open communication class has period 2. -->
<!-- On the other hand, $p_{00} = 1$ (and also $p_{NN} =  1$) and hence, the -->
<!-- states $0$ and $N$ are aperiodic because a return to each of these -->
<!-- states is possible in any number of steps (or the system can remain in -->
<!-- this state in any length of time). -->
<!-- ::: {.example} -->
<!-- **Example 28**. *In the random walk model on an infinite state space -->
<!-- $S = \{\ldots,-2,-1,0,1,2, \ldots \}$ in -->
<!-- Example [Example 25](#rwInfiniteS){reference-type="ref" -->
<!-- reference="rwInfiniteS"}, determine the period of each state.* -->
<!-- ::: -->
<!-- The entire state space is a single class. Note also that -->
<!-- $p^{(2n)}_{ii} > 0$ and $p^{(2n+1)}_{ii} = 0$ for $n = 1,2,\ldots$. -->
<!-- Therefore each state in the random walk on an infinite set $S$ is -->
<!-- periodic with period 2. -->
<!-- ::: {.example} -->
<!-- **Example 29**. *Suppose the states of the system are $\{1,2,3,4 \}$ and -->
<!-- the transition matrix is $$P = \begin{bmatrix} -->
<!--     0 & 1/2 & 0 & 1/2    \\ -->
<!--     1/4 & 0 & 3/4 & 0   \\ -->
<!--     0 & 1/3 & 0 & 2/3    \\ -->
<!--     1/2 & 0 & 1/2 & 0   \\ -->
<!-- \end{bmatrix}.$$ Determine the period of each state.* -->
<!-- ::: -->
<!-- The entire state space is a single class. Note also that -->
<!-- $p^{(2n)}_{ii} > 0$ and $p^{(2n+1)}_{ii} = 0$ for $n = 1,2,\ldots$. -->
<!-- Therefore each state in the random walk on an infinite set $S$ is -->
<!-- periodic with period 2. -->
<!-- ::: {#exampleLongTerm .example} -->
<!-- **Example 30**. *A Markov chain $X_0, X_1, \ldots$ on states $\{1,2\}$ -->
<!-- has the following transition matrix $$P = \begin{bmatrix} -->
<!--     0 & 1   \\ -->
<!--     1 & 0   \\ -->
<!--     %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix},$$* -->
<!-- 1.  *[\[example1\]]{#example1 label="example1"} Find the stationary -->
<!--     distribution(s) of this Markov chain.* -->
<!-- 2.  *Describe the long-term behaviour of the Markov chain. Does the -->
<!--     distribution of the chain tend to the stationary distribution(s) -->
<!--     found in [\[example1\]](#example1){reference-type="ref" -->
<!--     reference="example1"}.* -->
<!-- ::: -->
<!-- 1.  To find the stationary distribution -->
<!--     $\boldsymbol{\pi} = (\pi_{1},  \pi_{2})$, we need to solve -->
<!--     $$\left(\pi_{1},  \pi_{2}\right) \begin{bmatrix} -->
<!--         0 & 1   \\ -->
<!--         1 & 0   \\ -->
<!--         %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--         %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!--     \end{bmatrix} = \left(\pi_{1},  \pi_{2}\right),$$ and -->
<!--     $$\pi_{1} +  \pi_{2}  = 1.$$ This gives -->
<!--     $\boldsymbol{\pi} = (1/2, 1/2).$ -->
<!-- 2.  There is an equal chance of being in either state. Note that for any -->
<!--     initial probability distribution -->
<!--     $\boldsymbol{\mu} =  (\mu_{1},\mu_{2})$ with $\mu_{1}+\mu_{2}=1$, we -->
<!--     have -->
<!--     $$\boldsymbol{\mu} \cdot P=\boldsymbol{\mu} \cdot P^{3}=\boldsymbol{\mu} \cdot P^{5}=\ldots -->
<!--     =(\mu_{2},\mu_{1})$$ and -->
<!--     $$\boldsymbol{\mu} P^{2}=\boldsymbol{\mu} P^{4}=\boldsymbol{\mu} P^{6}=\ldots = (\mu_{1},\mu_{2}).$$ -->
<!--     The process does not settle down to an equilibrium position. Note -->
<!--     also that the chain is not aperiodic, i.e. each state is periodic of -->
<!--     period 2. The process does not conform to stationary in the long -->
<!--     run. -->
<!-- ::: {.example} -->
<!-- **Example 31**. *A Markov chain $X_0, X_1, \ldots$ on states $\{1,2\}$ -->
<!-- has the following transition matrix $$P = \begin{bmatrix} -->
<!--     1/3 & 2/3   \\ -->
<!--     2/3 & 1/3   \\ -->
<!--     %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--     %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!-- \end{bmatrix},$$ Answer the same questions as given in the Example -->
<!-- [Example 30](#exampleLongTerm){reference-type="ref" -->
<!-- reference="exampleLongTerm"}* -->
<!-- ::: -->
<!-- 1.  The process is finite and irreducible, so a unique stationary -->
<!--     distribution exists. To find the stationary distribution -->
<!--     $\boldsymbol{\pi} = (\pi_{1},  \pi_{2})$, we need to solve -->
<!--     $$\left(\pi_{1},  \pi_{2}\right) \begin{bmatrix} -->
<!--         1/3 & 2/3   \\ -->
<!--         2/3 & 1/3   \\ -->
<!--         %\vdots & \vdots & \vdots  & \vdots \\ -->
<!--         %p_{d1} & p_{d2} & p_{d3} & \dots  & p_{dn} -->
<!--     \end{bmatrix} = \left(\pi_{1},  \pi_{2}\right),$$ and -->
<!--     $$\pi_{1} +  \pi_{2}  = 1.$$ Solving the system of linear equations -->
<!--     gives $\boldsymbol{\pi} = (1/2, 1/2).$ -->
<!-- 2.  The chain is aperiodic. Therefore, according to the results from -->
<!--     Example [Example 22](#exampleStationary){reference-type="ref" -->
<!--     reference="exampleStationary"}, if follows that -->
<!--     $$\lim_{n \rightarrow \infty} p_{i1}^{(n)} = 1/2 > 0 , \text{ and }  \lim_{n \rightarrow \infty} p_{i2}^{(n)} = 1/2,$$ -->
<!--     which is independent of $i$. This is contrast to the process given -->
<!--     in Example [Example 30](#exampleLongTerm){reference-type="ref" -->
<!--     reference="exampleLongTerm"}, i.e. the process in this example -->
<!--     reaches the stationary probability distribution in the long run. -->
<!-- Main result {#main-result .unnumbered} -->
<!-- ----------- -->
<!-- The main result in this section can be stated as follows: -->
<!-- ::: {.theorem} -->
<!-- **Theorem 3**. *Let $P$ be the transition probability matrix of a -->
<!-- homogeneous discrete-time Markov chain $\{X_n \}_{n \ge 0}$. If the -->
<!-- Markov chain is* -->
<!-- -   *finite,* -->
<!-- -   *irreducible and* -->
<!-- -   *aperiodic,* -->
<!-- *then there is the unique probability distribution -->
<!-- $\boldsymbol{\pi} = (\pi_j)_{j\in S}$ such that -->
<!-- $$\lim_{n \rightarrow \infty} p^{(n)}_{ij} = \pi_j > 0, \text{ for any } j \in S$$ -->
<!-- and $$\sum_{j \in S} \pi_j = 1,$$ and this distribution is independent -->
<!-- of the initial state. Such probability distribution $\boldsymbol{\pi}$ -->
<!-- is called the **limiting probability distribution**. In addition, the -->
<!-- limiting distribution $\boldsymbol{\pi} = (\pi_j)_{j\in S}$ is the -->
<!-- stationary probability distribution of the Markov chain, i.e. it also -->
<!-- satisfies $\boldsymbol{\pi}  P  =\boldsymbol{\pi}$.* -->
<!-- ::: -->
<!-- ::: {.example} -->
<!-- **Example 32**. *Recall the Markov chain as given in -->
<!-- Example [Example 22](#exampleStationary){reference-type="ref" -->
<!-- reference="exampleStationary"}. The Markov chain is finite, irreducible -->
<!-- and aperiodic. We have also shown that -->
<!-- $$\boldsymbol{\pi} = \left( \frac{b}{a+b},  \frac{a}{a+b} \right)$$ is -->
<!-- the limiting distribution, i.e. -->
<!-- $$\lim_{n \rightarrow \infty} p_{ij}^{(n)} = \pi_j > 0 , \text{ for }j \in \{1,2\},$$ -->
<!-- independent of $i$. This limiting distribution is also the unique -->
<!-- stationary distribution of the Markov chain, which can be verified by -->
<!-- $$\left( \frac{b}{a+b},  \frac{a}{a+b} \right) \begin{bmatrix} -->
<!--     1-a & a   \\ -->
<!--     b & 1-b   \\ -->
<!-- \end{bmatrix} =  \left( \frac{b}{a+b},  \frac{a}{a+b} \right),$$ where -->
<!-- $0 < a,b < 1.$* -->
<!-- ::: -->
<!-- ::: {.example} -->
<!-- **Example 33**. *The above result can be applied to the NCD system -->
<!-- because it is finite, irreducible and aperiodic. Indeed, there is the -->
<!-- unique limiting probability distribution -->
<!-- $$\boldsymbol{\pi} = \left(\frac{(p-1)^2}{p^2 - p +1}, \frac{p - p^2}{p^2 - p +1}, \frac{p^2}{p^2 - p +1}\right),$$ -->
<!-- which is the stationary distribution of the chain. This gives the -->
<!-- long-term behaviour of the Markov chain, i.e. the probability of finding -->
<!-- the Markov chain in state $j$ is approximately $\pi_j$ independent of -->
<!-- the initial distribution.* -->
<!-- *For example, let $p =   0.8$ in the transition probability matrix $P$. -->
<!-- We compute several powers of $P$ as follows: $$\begin{aligned} -->
<!-- P &= \begin{bmatrix} -->
<!-- 0.2 & 0.8 & 0\\ -->
<!-- 0.2 & 0 & 0.8\\ -->
<!-- 0 & 0.2 & 0.8\\ -->
<!-- \end{bmatrix} -->
<!-- ,  & -->
<!-- P^2 &= \begin{bmatrix} -->
<!-- 0.2 & 0.16 & 0.64\\ -->
<!-- 0.04 & 0.32 & 0.64\\ -->
<!-- 0.04 & 0.16 & 0.8\\ -->
<!-- \end{bmatrix}, \\ -->
<!-- P^4 &= \begin{bmatrix} -->
<!-- 0.072 & 0.1856 & 0.7424\\ -->
<!-- 0.0464 & 0.2112 & 0.7424\\ -->
<!-- 0.0464 & 0.1856 & 0.768\\ -->
<!-- \end{bmatrix} -->
<!-- ,  & -->
<!-- P^8 &= \begin{bmatrix} -->
<!-- 0.0482432 & 0.1903514 & 0.7614054\\ -->
<!-- 0.04758784 & 0.1910067 & 0.7614054\\ -->
<!-- 0.04758784 & 0.1903514 & 0.7620608\\ -->
<!-- \end{bmatrix} \\ -->
<!-- P^{16} &= \begin{bmatrix} -->
<!-- 0.04761946 & 0.1904761 & 0.7619044\\ -->
<!-- 0.04761903 & 0.1904765 & 0.7619044\\ -->
<!-- 0.04761903 & 0.1904761 & 0.7619049\\ -->
<!-- \end{bmatrix} -->
<!-- ,  & -->
<!-- P^{32} &= \begin{bmatrix} -->
<!-- 0.04761905 & 0.1904762 & 0.7619048\\ -->
<!-- 0.04761905 & 0.1904762 & 0.7619048\\ -->
<!-- 0.04761905 & 0.1904762 & 0.7619048\\ -->
<!-- \end{bmatrix}.\end{aligned}$$* -->
<!-- ::: -->
<!-- The limiting probability distribution is -->
<!-- $$\lim_{n\rightarrow \infty} P^n= -->
<!--  \begin{bmatrix} -->
<!-- 0.04761905 & 0.1904762 & 0.7619048\\ -->
<!-- 0.04761905 & 0.1904762 & 0.7619048\\ -->
<!-- 0.04761905 & 0.1904762 & 0.7619048\\ -->
<!-- \end{bmatrix}.$$ -->
<!-- ### Applications of Markov chains to NCD systems {#applications-of-markov-chains-to-ncd-systems .unnumbered} -->
<!-- ::: {.example} -->
<!-- **Example 34**. *A no-claims discount system for motor insurance has -->
<!-- three levels of discount:* -->
<!--     *Level*     *1*     *2*     *3* -->
<!--   ------------ ------ ------- ------- -->
<!--    *Discount*   *0%*   *30%*   *50%* -->
<!-- *The rules for moving between these levels are given as follows:* -->
<!-- -   *Following a claim-free year, move to the next higher level, or -->
<!--     remain at level 3.* -->
<!-- -   *Following a year with one claim, move to the next lower level, or -->
<!--     remain at level 1.* -->
<!-- -   *Following a year with two or more claims, move to level 1, or -->
<!--     remain at level 1.* -->
<!-- *A portfolio consists of 10000 policyholders, of which* -->
<!-- -   *5000 policyholders are classified as good drivers. The number of -->
<!--     claims per year in this group is $\text{Poisson}(0.1)$.* -->
<!-- -   *5000 policyholders are classified as bad drivers. The number of -->
<!--     claims per year in this group is $\text{Poisson}(0.2)$.* -->
<!-- 1.  *Calculate $\Pr[N = 0]$, $\Pr[N = 1]$, and $\Pr[N \ge 2]$ for each -->
<!--     group.* -->
<!-- 2.  *Write down the transition probability matrix of this no-claims -->
<!--     discount system for each group.* -->
<!-- 3.  *Calculate the expected number of policyholders at each level for -->
<!--     each group once stability has been achieved.* -->
<!-- 4.  *Calculate the expected premium income per driver from each group -->
<!--     once stability has been achieved.* -->
<!-- 5.  *Calculate the ratio of the expected premium income per driver from -->
<!--     the group of good drivers to that from the group of bad drivers once -->
<!--     stability has been achieved.* -->
<!-- 6.  *Comments on the results obtained. Does this NCD system encourage -->
<!--     good driving?* -->
<!-- ::: -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="review-of-probability-theory.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="discrete-time-markov-chains.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-stochastic-processes.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/pairote-sat/SCMA469/blob/master/02-stochastic-processes.Rmd",
"text": null
},
"download": ["bookdownproj.pdf", "bookdownproj.epub"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
